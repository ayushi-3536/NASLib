{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJJAv3iRprTp"
      },
      "source": [
        "# NASLib tutorial and intro to exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KMjX9kAprTt"
      },
      "source": [
        "[NASLib](https://github.com/automl/NASLib) is framework that was built in order to facilitate neural architecture search (NAS) research and development. Please refer to the slides and to the [NAS survey paper](https://arxiv.org/abs/1808.05377) for more details. At the high-level NASLib consists of 4 main building blocks which (can) interact with each other:\n",
        "- search spaces (cell search space, hierarchical, ...)\n",
        "- optimizers (one-shot/weight-sharing optimizers, black-box optimizers)\n",
        "- predictors (performance estimators that given an architecture as input, output its performance)\n",
        "- evaluators (run the architecture search loop and the final network training pipeline)\n",
        "\n",
        "**NOTE: NASLib is currently under development. This exercise is meant to be beneficial for both students and the NASLib developers. In case of any issues or bugs please contact us and we will try to fix those. If you are interested in working to extend the library please contact Arber.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0GspvzZprTu"
      },
      "source": [
        "![naslib-overview.png](attachment:naslib-overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXQcQoXsprTu"
      },
      "source": [
        "## Installation and setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lthaPC_prTv"
      },
      "source": [
        "To setup your environment and install NASLib follow these steps:\n",
        "```\n",
        "git clone -b dllab22 https://github.com/automl/NASLib/\n",
        "cd NASLib\n",
        "conda create -n naslib_exercises python=3.7\n",
        "conda activate naslib_exercises\n",
        "pip install --upgrade pip setuptools wheel\n",
        "pip install -e .\n",
        "pip install jupyter gdown\n",
        "source scripts/download_data.sh nb201 cifar10\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone  https://ayushi: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@github.com/ayushi-3536/NASLib.git\n"
      ],
      "metadata": {
        "id": "EQ2J1e6CpunM",
        "outputId": "a9831d8f-ca22-4ca1-f2ae-adcbb6d53cdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NASLib' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd NASLib/"
      ],
      "metadata": {
        "id": "NZaWUq00qdT2",
        "outputId": "8f6eefe1-297b-4f7e-bc30-d70c5852712c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NASLib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout dllab22"
      ],
      "metadata": {
        "id": "KeVNUqtnxVhJ",
        "outputId": "dc971f54-a135-46e0-9b59-ef72eafcf797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'dllab22'\n",
            "Your branch is up to date with 'origin/dllab22'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FYblol5VxYs_",
        "outputId": "9c5d5abf-fbb9-40fa-ae0c-f2aded348a09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install -e .\n",
        "!pip install jupyter gdown"
      ],
      "metadata": {
        "id": "vCm9PPGLp6br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDIKx6ecprTv"
      },
      "source": [
        "## Cell search spaces in NASLib "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wT7RVOQprTv"
      },
      "source": [
        "The search space representation is of primary importance for NASLib in ensuring that optimizers and search spaces can be combined in a variety of ways. The predominant way of representing NAS search spaces is the directed acyclic graph (DAG). \n",
        "In order to accomplish the aforementioned functionality of search spaces and computational graphs, we inherit in our basic graph classes from both [PyTorch](https://pytorch.org/) and [NetworkX](https://github.com/networkx/networkx). The latter is a well-maintained and tested Python package for graph creation and manipulation, where node and edge attributes can be arbitrary Python objects. This framework allows us to represent multiple layers of graphs on top of the computational graph, allowing us to treat nodes and edges both as primitive operations (e.g. convolution), but also nested graph-structures such as a DARTS cell, to create e.g. macro architectures of stacked cells. NetworkX allows to easily construct the search space via `add_node`, `remove_node`, `add_edge`, `remove_edge`, or traverse the topologically sorted graph in the forward pass of the PyTorch module using `networkx.algorithms.dag.topological_sort`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCcoFcCnprTw"
      },
      "source": [
        "### Case study: NAS-Bench-201"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxPpwvhNprTx"
      },
      "source": [
        "[The NAS-Bench-201](https://openreview.net/forum?id=HJxyZkBKDr) is a tabular benchmark, i.e. a benchmark where you can simply query (already has been trained) the performance and other metrics of a specific architecture in the search space given that as an input. Its search space consists of a single normal cell which is replicated multiple times in a macro architecture interleaved by manually defined resnet-like reduction cells. The cell topology is fixed in the cell and consists of:\n",
        "- 1 input, 2 intermediate and 1 output node;\n",
        "- a summation operation on each of the intermediate and output nodes;\n",
        "- 5 operation choices in each of the edges connecting 2 nodes\n",
        "    - 'none'\n",
        "    - 'skip_connect'\n",
        "    - 'nor_conv_1x1'\n",
        "    - 'nor_conv_3x3'\n",
        "    - 'avg_pool_3x3'\n",
        "    \n",
        "For an example on how this search space is defined using the NASLib terminology, refer  [here](https://github.com/automl/NASLib/blob/predictors/naslib/search_spaces/nasbench201/graph.py)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd NASLib"
      ],
      "metadata": {
        "id": "OsCQFFXQs-bW",
        "outputId": "e8dfaee4-ac87-41a4-f259-0ce814571b6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NASLib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "SwqyhvY8tF7G",
        "outputId": "db4edf08-34ca-4711-ef56-66c97b7b4dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NASLib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpnPKxgSprTy"
      },
      "source": [
        "![nb201.png](attachment:nb201.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yct1R3cmprTy",
        "outputId": "84152946-8d61-46f6-a9af-433563536315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Update function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
            "Update function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
            "Update function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n"
          ]
        }
      ],
      "source": [
        "from naslib.search_spaces import NasBench201SearchSpace as NB201\n",
        "\n",
        "# instantiate the search space object\n",
        "search_space = NB201()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hqYELR9prT0"
      },
      "source": [
        "## Black-box optimizers in NASLib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1-smg4prT0"
      },
      "source": [
        "After learning about the search space object, now we can add the other component of NAS: the NAS optimizer which you will use to search for an optimal architecture in that search space. A search space graph object can be interpreted in different ways depending on the type of optimizer being used. Here is the point where the search space and optimizer objects interact by parsing information from each other. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uglSLog2prT0",
        "outputId": "917f39e1-f401-49b8-df1d-91519c48fcd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(config_file='/content/NASLib/naslib/benchmarks/nas_predictors/discrete_config.yaml', dist_backend='nccl', dist_url='tcp://127.0.0.1:8888', eval_only=False, gpu=None, model_path=None, multiprocessing_distributed=False, opts=[], rank=0, resume=False, seed=0, world_size=1)\n"
          ]
        }
      ],
      "source": [
        "# import some utilities and parse the configuration file\n",
        "import logging\n",
        "\n",
        "from naslib.utils import utils, setup_logger, get_dataset_api\n",
        "\n",
        "# This will read the parameters from the default yaml configuration file, which in this \n",
        "# case is located in NASLib/naslib/benchmarks/nas_predictors/discrete_config.yaml.\n",
        "# You do not have to change this but you can play around with its parameters.\n",
        "config = utils.get_config_from_args(config_type=\"nas_predictor\")\n",
        "utils.set_seed(config.seed)\n",
        "utils.log_args(config)\n",
        "\n",
        "logger = setup_logger(config.save + \"/log.log\")\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GJgHftG0prT1"
      },
      "outputs": [],
      "source": [
        "from naslib.optimizers import RegularizedEvolution as RE\n",
        "\n",
        "# instantiate the optimizer object using the configuration file parameters\n",
        "optimizer = RE(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts//download_data.sh nb201 cifar10"
      ],
      "metadata": {
        "id": "FNLHQo-7tS2n",
        "outputId": "518c88e9-e4ed-474d-e504-ab64c5f083a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset = cifar10\n",
            "search_space = nb201\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sh8pEhdrgZ97-VFBVL94rI36gedExVgJ\n",
            "To: /content/NASLib/naslib/data/nb201_cifar10_full_training.pickle\n",
            "100% 117M/117M [00:00<00:00, 199MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "I6EfzlvtwdWx",
        "outputId": "e07ca0cb-f7ec-498e-b3c6-506ba7ad05a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NASLib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch"
      ],
      "metadata": {
        "id": "GHEQyje7wz6L",
        "outputId": "e62de070-683c-4e5f-d38b-a712b5e58b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mDevelop\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout dllab22"
      ],
      "metadata": {
        "id": "k53jyN36w2DN",
        "outputId": "b6f10cce-bce4-4a3a-cece-f755c83aa2f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'dllab22' set up to track remote branch 'dllab22' from 'origin'.\n",
            "Switched to a new branch 'dllab22'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd scripts"
      ],
      "metadata": {
        "id": "6NvkMB0owfOz",
        "outputId": "4647826a-3765-4ffb-e76c-3c18bd748764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NASLib/scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download_data.sh nb201 cifar10"
      ],
      "metadata": {
        "id": "wDZmvN21wPFO",
        "outputId": "a5603154-df37-4f5a-87ea-5f8d26581b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "bash: download_data.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIxLp-lOprT1"
      },
      "source": [
        "After parsing the configuration file and instantiating the NAS optimizer and search space objects, we have to adapt the search space based on the optimizer type. \n",
        "A black-box optimizer such as Random Search will sample single architectures using the `sample_random_architecture` method of the search space object (e.g. by sampling one operation at each graph edge from the operation choices in NAS-Bench-201) throughout the optimization process.\n",
        "On the other hand most one-shot optimizers, such as [DARTS](https://arxiv.org/abs/1806.09055), will interpret a set of operation choices on an edge as a `MixedOp` and assign an appropriate number of architectural weights (between 0 and 1, such that the sum is 1) to the outputs of each operation in order to obtain the continuous relaxation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiTQvHDIprT1"
      },
      "source": [
        "Download the NAS-Bench-201 data from https://drive.google.com/file/d/17EBlTidimMaGrb3fE0APbljJl-ocgfs4/view?usp=sharing and place it in `NASLib/naslib/data/`. Alternatively run ```source scripts/download_data.sh nb201 cifar10```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0_q6UzIpprT2"
      },
      "outputs": [],
      "source": [
        "# this will load the NAS-Bench-201 data (architectures and their accuracy, runtime, etc).\n",
        "dataset_api = get_dataset_api(config.search_space, config.dataset)\n",
        "\n",
        "# adapt the search space to the optimizer type\n",
        "optimizer.adapt_search_space(search_space, dataset_api=dataset_api)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DuQL67xprT2"
      },
      "source": [
        "## Running the search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19zOCQE3prT2"
      },
      "source": [
        "Now the only step left is to run the search. Fro this we will use the `Trainer` object in NASLib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tGO0BIy8prT2",
        "outputId": "69fa2490-52ba-4570-c0fd-9692dc4b2ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[05/15 12:53:57 nl.defaults.trainer]: \u001b[0mparam size = 0.000000MB\n"
          ]
        }
      ],
      "source": [
        "from naslib.defaults.trainer import Trainer\n",
        "\n",
        "# since the optimizer has parsed the information of the search space, we do not need to pass the search\n",
        "# space object to the trainer when instantiating it.\n",
        "trainer = Trainer(optimizer, config, lightweight_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "udK5wulWprT3",
        "outputId": "1b96d216-3146-4ce8-db5d-4b699579cee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[05/15 12:54:03 nl.defaults.trainer]: \u001b[0mStart training\n",
            "\u001b[32m[05/15 12:54:03 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[05/15 12:54:03 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[05/15 12:54:03 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
            "\u001b[32m[05/15 12:54:03 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 1\n",
            "\u001b[32m[05/15 12:54:03 nl.defaults.trainer]: \u001b[0mEpoch 0, Anytime results: {'cifar10-valid': {'train_losses': [1.933049173965454, 1.6435673345184325, 1.5047246058654786, 1.3834531283569336, 1.286956615371704, 1.1839038812637328, 1.1257015887069701, 1.0640545072746277, 1.008200834274292, 0.9675810929870605, 0.9294775085449218, 0.8978886487960815, 0.8689233894729614, 0.835043701877594, 0.8149485078430175, 0.7993172987365723, 0.7647924902915955, 0.7442652308464051, 0.7303601633262634, 0.7206139094543457, 0.700282748222351, 0.689298511505127, 0.671164067955017, 0.659271063709259, 0.6536657622528076, 0.644965997467041, 0.6267636371994019, 0.6300651649665833, 0.6204490633773804, 0.6097293877315522, 0.6010741853713989, 0.5971751938438415, 0.5914653929138184, 0.5821332813644409, 0.5756466757678985, 0.5765612985229492, 0.5747497636222839, 0.5639631831932068, 0.5726355790519715, 0.5540831290435791, 0.544603844833374, 0.538719157447815, 0.5455957692337036, 0.5412485385322571, 0.5337968168449402, 0.5338672785758972, 0.5202459384346009, 0.5235530172157288, 0.5228768538284302, 0.5111833030891418, 0.5069599935722351, 0.4985576431465149, 0.5061389976501465, 0.5057560344696045, 0.4975048216819763, 0.4996471509456635, 0.49808415460586547, 0.48002091426849364, 0.4895668503761291, 0.47426448446273806, 0.47582018551826477, 0.4727789403152466, 0.4713308194351196, 0.4654431737613678, 0.4631043137359619, 0.4602999722862244, 0.46426936804771424, 0.45147063561439515, 0.4586259645652771, 0.4530562820339203, 0.4525101449012756, 0.44904636945724485, 0.4421387914657593, 0.44143003540992737, 0.43449196798324585, 0.429807522983551, 0.4321302970981598, 0.42644655586242675, 0.42557577180862427, 0.4184540640449524, 0.4111059604167938, 0.41888580284118654, 0.4107645316886902, 0.4032634139728546, 0.3989506572628021, 0.40599107765197756, 0.4013190036678314, 0.4004554117488861, 0.39978235445976257, 0.3918292625141144, 0.38755730080604556, 0.3802079680633545, 0.37822834414482115, 0.3721178003501892, 0.36796443832397463, 0.3624983962631226, 0.3639153636550903, 0.3588394504070282, 0.3580660499668121, 0.35965268224716185, 0.3529567024803162, 0.34759018639564515, 0.3482636020565033, 0.3410040765285492, 0.34433123145103456, 0.3332363390350342, 0.3343586909866333, 0.33191886186599734, 0.32960432033538817, 0.3258754188966751, 0.3183572149848938, 0.31825309366226195, 0.31322453734397887, 0.3127961633872986, 0.3112921525478363, 0.3073782663154602, 0.305751911945343, 0.2877358784770966, 0.29309685286521914, 0.28946088859558106, 0.2847796403694153, 0.2818057761096954, 0.2738027645301819, 0.2745120370388031, 0.2675325034427643, 0.25867299350261685, 0.25669949093818667, 0.25210444190979003, 0.2556096069812775, 0.24657657341957093, 0.2538064208316803, 0.24273306458473207, 0.23724620116710662, 0.23558641786575318, 0.22658208312988282, 0.22561613562583924, 0.2175565439891815, 0.2160201839542389, 0.2115384543132782, 0.2083314898300171, 0.20276337226867674, 0.19796098860263825, 0.19677157368183135, 0.18868987060546874, 0.18665040482521056, 0.1838897520160675, 0.16965494362831116, 0.1692909675502777, 0.16223212686538696, 0.16421128366708757, 0.15417879284381866, 0.15248106106758116, 0.15179483103752137, 0.14301283118724822, 0.13277943490028382, 0.1390109401512146, 0.1331410027551651, 0.13165652403831482, 0.1251206089258194, 0.12002688515186309, 0.11082318178653718, 0.11014242079257965, 0.10549740042209625, 0.10062637070178986, 0.09586149839639664, 0.09223287776947021, 0.09235902231693267, 0.0871694572353363, 0.07851553828954697, 0.07523862042188645, 0.07313915983438492, 0.07498600163459777, 0.0686159856414795, 0.06791985613346099, 0.06255743339061737, 0.060810287736654284, 0.05605678023815155, 0.051493860071897504, 0.05079198920726776, 0.04651469718694687, 0.04590114063620567, 0.04247044575214386, 0.042684141470193865, 0.04604436099410057, 0.03932782050609589, 0.040091955519914624, 0.03989724842786789, 0.03675567040979862, 0.03643383831262589, 0.03524160267472267, 0.03457283695936203, 0.03541027491092682, 0.03261254559040069, 0.031013704183101655, 0.03276500719666481, 0.03511530919075012, 0.03020550874441862, 0.0300822855424881, 0.03165007179617882, 0.03257577588319779], 'eval_losses': [1.853276103477478, 1.9825896265411378, 1.7628808596038819, 1.4096657558441161, 1.4998609498214721, 1.383873239364624, 1.7190013084411622, 1.262677116317749, 1.3637014740371705, 1.2649028635406494, 1.4128168886947632, 1.3575548432922364, 1.1706336224746705, 1.4476164538574219, 1.085583380355835, 1.4277293459320068, 1.845134800338745, 1.1361894588470458, 1.7432173997497558, 1.047818994617462, 1.0644127959823608, 1.2358372732162475, 0.7923082675361633, 1.8984723958587646, 1.227219734992981, 1.2374632009124755, 1.8372280381774901, 3.3594971089172363, 1.8625118659973146, 1.4339509927368164, 1.194370693397522, 0.8332742945289612, 0.9140051091003418, 1.284939202232361, 0.9161269886398316, 1.3480814905166627, 1.0398822765731812, 1.2605186191177369, 0.8410207567596436, 0.9902118959808349, 0.7680131269073487, 1.2281337160491943, 1.0896810879898071, 0.8184736426544189, 0.8645902954673768, 1.7687164207839965, 0.8077349285507203, 1.207072050819397, 1.085643136177063, 0.9497915023803711, 1.2137908243942261, 1.0207273559761048, 1.5429754879379272, 0.8329608443450928, 0.8357771710777283, 0.7830682307052612, 1.1425826570892335, 0.7564472578811645, 0.87835656539917, 1.4388655133819581, 1.4618375290679932, 0.8127754815483094, 0.9564275854301453, 1.4416781497192384, 1.1074947327041627, 0.9531717235565186, 0.8289349126243591, 1.2754418816375732, 1.1349834226608277, 1.7311271363067626, 0.8819717637634278, 0.933219065990448, 0.784826379699707, 0.8595322378540039, 0.7136831183815002, 1.2118772090911865, 0.6783367133140564, 0.8996105028533935, 1.4115132711029053, 0.8072059643936157, 1.142419045753479, 0.7940620897483825, 0.878292625541687, 1.0021474601745606, 0.7369408583450318, 0.8604336462402343, 0.8097284167098999, 0.8572968395805359, 0.7299011550521851, 0.8498886232185364, 1.0990450579833984, 0.9377336064529419, 1.596045492286682, 0.7873356300163269, 0.9096064290618896, 0.8921308458900452, 0.847033512058258, 0.8047578199577331, 0.7195415874481201, 0.7557898583221435, 0.792297578868866, 0.6501456667709351, 1.2255437806701661, 0.7569386186790467, 0.8863202992630005, 0.6395072632789612, 0.7113047681808472, 0.6376822698974609, 0.7149502280426026, 0.686346060962677, 0.6167438936614991, 1.1942690195083618, 0.8190234329223632, 0.6529788191604614, 0.6655721626281739, 1.0146758702278138, 0.6935151036643982, 0.7063728413581848, 0.6997902191543579, 0.6693120042610169, 0.7805671958732605, 0.7888907621002197, 0.7633749666404724, 0.8210488289070129, 0.5793331812858582, 0.6228814896965027, 0.9085615486526489, 0.6486815407371521, 0.7352733605957031, 0.6726218161964417, 0.729565166721344, 0.7221049830436707, 0.6092571593093872, 0.9904315916442871, 0.7177288776016235, 0.8044267671012878, 0.7301563682746888, 0.5891368518066407, 0.6938168054580689, 0.6769888493728637, 0.6718026449966431, 0.6439840829849243, 0.6875117469215393, 0.692921744556427, 0.625256512184143, 0.6981553867149353, 0.636772664604187, 0.6946889575767518, 0.5836456839847565, 0.6240633099842071, 0.5985315733528137, 0.6856615443611145, 0.5918414127731323, 0.5658171324920654, 0.6106313031578064, 0.5521215028190612, 0.649893540172577, 0.5895428963470459, 0.5434328539276123, 0.597798399181366, 0.5711516212654114, 0.5831282989120483, 0.5978009645080566, 0.5461356635665894, 0.5734788090133667, 0.5795820503044128, 0.6116492777252197, 0.5694089279842377, 0.568785023059845, 0.5618854899406434, 0.5635403636169434, 0.5830152307128906, 0.5711482059288024, 0.5568729959487915, 0.5673009009170532, 0.5674946151924133, 0.5650128768253326, 0.5699412359237671, 0.5753453620719909, 0.5744260936355591, 0.5668122173500061, 0.5662241883659362, 0.5645727476119995, 0.5703306195449829, 0.55752571767807, 0.5635156677246094, 0.5574384237861634, 0.5620425259971619, 0.5665751437950134, 0.5677274066352844, 0.5643322320556641, 0.5647553562068939, 0.560993676662445, 0.5646405374145508, 0.5617116917800903, 0.5623938551521301, 0.5640535704803467, 0.5687453405570984, 0.5678173172187805, 0.5599888938522339, 0.5531601202487946], 'train_acc1es': [26.715999996337892, 38.55199999389649, 44.867999986572265, 49.551999997558596, 52.93199998535156, 57.03999998657227, 59.395999986572264, 61.803999990234374, 63.77599999511719, 65.5080000024414, 66.80799999023438, 68.05600001464843, 69.10000000732421, 70.80000001953125, 71.12400000976562, 71.73999997802734, 72.99600001708984, 74.06800000244141, 74.50400000732422, 74.81999999511719, 75.55599999267578, 75.58799998779297, 76.3640000024414, 76.87199998535156, 77.36799999023438, 77.53200000732421, 78.02399999755859, 78.13200000488281, 78.47199998291016, 78.72400001464844, 78.85999998291015, 79.03199999023437, 79.55599997314454, 79.72399998535157, 79.68400001220704, 79.78799999267578, 79.82000001708984, 80.31599997314453, 79.54799997558594, 80.78799997070313, 80.95199999511719, 81.33600001220704, 80.94, 81.30399999755859, 81.52799998779297, 81.34800001953126, 82.10399998291015, 81.69999999267579, 81.85199998779296, 82.32399998535156, 82.25200001953125, 82.668, 82.43999998291015, 82.53599997802735, 82.84800000244141, 82.42, 82.71199998291016, 83.21999997558594, 82.968, 83.38799997558594, 83.34000001220703, 83.65999997070313, 83.54800001464844, 83.93199997070313, 83.74000000976562, 84.13199997070312, 83.65600000976562, 84.11200001220703, 84.13999999267578, 84.26400001953125, 84.1080000024414, 84.47599998291015, 84.58799998046875, 84.64400001220703, 84.67999997070312, 85.06400001708984, 84.86800000488282, 85.21999998779297, 85.27600001220704, 85.3680000024414, 85.48799999755859, 85.21999997558594, 85.69999997314453, 85.91600000488282, 86.0920000048828, 85.91999997070313, 86.156, 86.09200001953126, 85.9760000048828, 86.31199998535156, 86.44800001953125, 86.69600001220704, 86.56, 87.04, 87.24000001953125, 87.21600001220703, 87.17199999511719, 87.2360000024414, 87.49999999755859, 87.61200001464844, 87.8880000024414, 88.02400000732422, 87.86000001464843, 87.99199998779297, 87.80399999267578, 88.28000001464844, 88.24800000488281, 88.35599999267578, 88.45199997070313, 88.64799998046875, 88.68399999755859, 88.9880000048828, 88.83599997314452, 88.99999999023437, 89.11599999755859, 89.39600000244141, 89.10799998535157, 90.0199999975586, 89.63599999023438, 89.7720000024414, 90.09999999267578, 90.0320000024414, 90.39999998291016, 90.22399998046875, 90.52799999023438, 90.91599999023437, 90.91599999267578, 91.01999998779297, 91.03999997314453, 91.40399998779297, 91.05199998291016, 91.46799998779296, 91.79999998291015, 91.61199999511719, 92.19599998291015, 92.1319999975586, 92.37599997314453, 92.32799998779296, 92.47999999267579, 92.64799997070313, 92.92399998291016, 93.14799999023438, 93.08800001708984, 93.30799998046875, 93.28399997070312, 93.53199998535156, 93.95999998535156, 94.15199997314453, 94.27999999267578, 94.18800001708985, 94.63999997802735, 94.72400001953125, 94.64800001953125, 94.86399997314453, 95.37199997802735, 95.25599998046874, 95.43599997802734, 95.28400001708984, 95.60800001220703, 95.74800001953125, 96.18000000976562, 96.15999998046875, 96.49200001464844, 96.61599997070313, 96.72800000976562, 96.84000001220703, 96.86400001708985, 97.03600000976563, 97.47200000976562, 97.50800000732421, 97.54000000976562, 97.62400001220703, 97.65200001220703, 97.76400000976562, 98.01600000732422, 98.02800000976562, 98.15200001464844, 98.39200000488282, 98.38400000732422, 98.50800001464843, 98.6040000048828, 98.7080000048828, 98.7320000048828, 98.55600000976563, 98.8360000048828, 98.82000000976562, 98.80400000488281, 98.91600000244141, 98.91600000244141, 98.99200000488281, 99.0000000048828, 99.02400000488281, 99.0960000024414, 99.18800000732422, 99.09200000244141, 99.01200000976563, 99.112, 99.23200000244141, 99.16400000244141, 99.116], 'eval_acc1es': [29.95599999511719, 34.831999998168946, 37.55999999755859, 48.356000002441405, 47.83199999633789, 50.7039999975586, 46.04000000366211, 55.82000000732422, 53.639999989013674, 56.139999998779295, 54.5159999987793, 55.676, 60.82399999267578, 54.960000001220706, 65.24799999267579, 56.69599999633789, 50.200000009765624, 63.07199999267578, 56.228000008544925, 64.50000001220702, 66.18799998535157, 60.80799998901367, 72.28399999511718, 49.7599999987793, 62.191999989013674, 59.923999995117185, 54.55200000854492, 36.352000009765625, 52.251999990234374, 60.30799999633789, 64.57999997558593, 71.59600000976563, 69.63200000732422, 62.28000001708985, 70.32799997558594, 60.78399998046875, 67.55999997802735, 63.82799997802734, 72.32399997070313, 67.67599997802735, 74.74799998779297, 63.70799998291016, 67.71199999511718, 73.57600001220703, 71.36000001708985, 52.228000006103514, 73.23599998291016, 63.84400001953125, 66.46399999633789, 70.40799999267578, 65.94399998046875, 68.784, 62.70000001464844, 72.89199997314454, 73.664, 74.50399999511718, 66.22000000244141, 75.77200001953125, 73.03599999755859, 62.29600000732422, 58.65999998657227, 73.96799997558594, 71.82800000732422, 62.524000002441404, 69.23200001464843, 71.10799997314453, 73.82799998535157, 64.84399999267578, 64.89600000732422, 57.97199999389648, 72.56400001953125, 71.7919999951172, 74.75599999755859, 73.2719999975586, 77.16000000976562, 65.59999998046875, 77.82399997314454, 71.83200001220703, 64.51599999023438, 74.01600001708984, 67.71999997314452, 75.51200001708985, 73.58000000976563, 70.11599997070313, 75.7039999975586, 73.46000000488282, 74.50800000488282, 73.21599998291016, 77.988, 73.65199999755859, 68.84399997314453, 72.25199998291015, 59.984000002441405, 76.34800000244141, 71.31600000732422, 74.85599999023438, 74.28000000488281, 74.60399999267578, 77.16400000732422, 76.48799999267578, 75.96399999267578, 79.26799998535157, 64.36800000976562, 76.85599998535156, 73.66799998291016, 79.89199998046875, 79.176, 79.89600000732422, 78.06400001464844, 78.42799998535156, 81.04399997558593, 67.30000001953125, 76.20800000732422, 79.68799998779296, 79.75999998046875, 72.96399999023437, 79.08399999267579, 78.97599998046876, 79.23599997314453, 79.13999998291015, 77.816, 76.524, 78.06000001708985, 76.54799998535157, 81.57199997314453, 81.24400001220702, 75.27599999023437, 80.54399998535156, 79.052, 80.38000000488282, 78.8920000024414, 79.83199997558594, 81.53199997802734, 73.84000001953125, 79.89199997314454, 77.56800000488282, 79.63599997558593, 82.38400001464844, 80.96399997070313, 81.15199998291016, 81.20000001953125, 81.72800000976562, 81.13199997558594, 81.27199998291016, 82.62000001220703, 80.55200000732422, 82.01999997070313, 81.62399997314454, 83.62000000488281, 82.81199999267578, 83.42400001708984, 81.61600001708985, 83.80799997802734, 84.21600000488282, 83.50800001708984, 84.52799999023438, 82.46399999267578, 83.76400000488282, 84.95200001464843, 83.99600001953125, 84.69599999755859, 84.68400001953125, 84.36400001708985, 85.46400000244141, 85.20000000976563, 85.02399997070313, 84.59599997070312, 85.32000000732423, 85.34799999755859, 85.65200000976563, 85.86000000732422, 85.70000000732422, 85.7479999975586, 85.96800001953125, 85.9560000024414, 86.10400001708985, 86.11999999023438, 86.18800001220703, 86.18400001220704, 86.39200000488282, 86.2840000024414, 86.5279999975586, 86.35200000976562, 86.324, 86.61999998291016, 86.59199997314452, 86.82, 86.6919999975586, 86.73199998291015, 86.67199999023437, 86.77600000732421, 86.76800000976563, 86.73600000976562, 86.776, 86.80400001708985, 86.78800000732421, 86.70799999755859, 86.7280000024414, 86.69600001220704, 86.74800001708985, 86.81], 'cost_info': {'flops': 19.57953, 'params': 0.157306, 'latency': 0.01700269443946972, 'train_time': 9.658029774824778}}}\n",
            "\u001b[32m[05/15 12:54:03 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 99.11600, 0.00000, Validation accuracy: 86.81000, 0.00000\n",
            "\u001b[32m[05/15 12:54:03 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:03 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy (top1, top5): 99.11600, 0.00000, Validation accuracy: 86.81000, 0.00000\n",
            "\u001b[32m[05/15 12:54:03 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:03 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy (top1, top5): 99.11600, 0.00000, Validation accuracy: 86.81000, 0.00000\n",
            "\u001b[32m[05/15 12:54:03 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:04 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy (top1, top5): 99.11600, 0.00000, Validation accuracy: 86.81000, 0.00000\n",
            "\u001b[32m[05/15 12:54:04 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:04 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 5\n",
            "\u001b[32m[05/15 12:54:04 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:04 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:04 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:04 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:05 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:05 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:05 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:05 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:05 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 9\n",
            "\u001b[32m[05/15 12:54:05 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:05 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:05 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:05 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:05 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:05 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:05 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:05 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:06 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 13\n",
            "\u001b[32m[05/15 12:54:06 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:06 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:06 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:06 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:06 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:06 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:07 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:07 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:07 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:07 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:07 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:07 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:07 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 19\n",
            "\u001b[32m[05/15 12:54:07 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:07 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:08 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:08 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:08 nl.defaults.trainer]: \u001b[0mEpoch 20, Anytime results: {'cifar10-valid': {'train_losses': [1.7726301501464843, 1.2632741512680055, 1.0350315731430053, 0.9116640870666504, 0.8138554672622681, 0.7384605021095276, 0.6771002819061279, 0.6386288579177857, 0.5977942006492615, 0.566234061164856, 0.5407228638648987, 0.5169966495037079, 0.5059799674415588, 0.49211094816207884, 0.47442392370224, 0.4547991913414001, 0.45366372842788694, 0.44375011929512026, 0.4291874082946777, 0.4207805109882355, 0.4070393644332886, 0.4128223983192444, 0.3933644982147217, 0.3912780677032471, 0.39287704973220827, 0.37701327048301697, 0.3703632626247406, 0.36252842462539675, 0.3545865700340271, 0.36439799418449403, 0.35175954774856566, 0.352374792804718, 0.3494409676837921, 0.34050146408081056, 0.33357596693992614, 0.3253499046516418, 0.3226811494445801, 0.32070203971862793, 0.32613728584289553, 0.3122583133125305, 0.31861459453582763, 0.29858239007949827, 0.3031420729446411, 0.29814767778396606, 0.30188757351875306, 0.2935050654029846, 0.2895284996509552, 0.29574748558044434, 0.28127229190826414, 0.2788175543785095, 0.2840972238445282, 0.27199181344032286, 0.27290838750839236, 0.26574461709022523, 0.27254494309425353, 0.2622075592327118, 0.2729633536148071, 0.2605763591194153, 0.258357668504715, 0.23966920917510987, 0.2618053030633926, 0.25782158448696135, 0.24394119193077088, 0.2535585989665985, 0.2447747621011734, 0.239952808508873, 0.24004719898223878, 0.23840140478134156, 0.24016718745708465, 0.23675682633399964, 0.22616518263816834, 0.21403014478683471, 0.22661552424430847, 0.22467548847675323, 0.20833352692604065, 0.21816086867809295, 0.21417523289680482, 0.21433385649681092, 0.20875024881839752, 0.20629779204368592, 0.1999822235584259, 0.20519787709236145, 0.1970332114267349, 0.19073082847595216, 0.19611911709308624, 0.1894867743396759, 0.19472460577487946, 0.17954142199993134, 0.17535858225107193, 0.18349442404270172, 0.17372775327205658, 0.17052123005867004, 0.17219759708881377, 0.1724909539937973, 0.16931497011661528, 0.16221350789546968, 0.1642095751476288, 0.1597701438188553, 0.15603769296169281, 0.15719229858636857, 0.15613695202350616, 0.14955226908683777, 0.14637396189451218, 0.14552984980583192, 0.1412839818239212, 0.14201841655254363, 0.14395703977823257, 0.13532956699371337, 0.12968134902000428, 0.1280316431903839, 0.1239628472828865, 0.11874924341201783, 0.12164036078929902, 0.11990649643421174, 0.1160284399986267, 0.11443230902433395, 0.11033453067302704, 0.10941301799297333, 0.10761771523952485, 0.10543239567756653, 0.09001801859736443, 0.10175345405340194, 0.09055124494314194, 0.08845513292551041, 0.08165875105142593, 0.08407168956756592, 0.08406028326034545, 0.08446159843683243, 0.08083324142694473, 0.07214234795331954, 0.07961240248322488, 0.07521961565852166, 0.07206464721918106, 0.06412235758781433, 0.05742816984653473, 0.06045057648539543, 0.05137665984988213, 0.049202715791463854, 0.049984927271604536, 0.04855161390900612, 0.047922370524406434, 0.043593682114481926, 0.039393371309041976, 0.03481456549406051, 0.03561475551605225, 0.03113965147972107, 0.027730674822330475, 0.03230222392678261, 0.02651573492050171, 0.027377016479969023, 0.022964896864891053, 0.022539460213184356, 0.023155455704927445, 0.020795817275047302, 0.019449235087633132, 0.016223811691999435, 0.015722117793262005, 0.014958812029063701, 0.012903136225938796, 0.011836160101294518, 0.010960419072806836, 0.011182013713121414, 0.010827825953662395, 0.009900919553041457, 0.008297104788422584, 0.0077525005152821545, 0.006521991423070431, 0.006128952867686749, 0.006098052520677447, 0.005712904340922833, 0.00588519031316042, 0.004925194225162268, 0.0043670635037124155, 0.004761073724329472, 0.0037600692857801916, 0.004550940673947334, 0.004170487490743399, 0.004020287989675998, 0.004145785541385412, 0.003908578156679869, 0.003949201945364475, 0.003207805385775864, 0.003279204101189971, 0.003943141021057963, 0.0038551958274841308, 0.0034629081816226243, 0.0034469756795093417, 0.0031426940720528365, 0.0034230355936288835, 0.0032149739553034304, 0.003457264813706279, 0.0033051802176237107, 0.003482307368069887, 0.003406692513898015, 0.0032044207777082922, 0.0028920193284004926, 0.003023021241724491, 0.0034502618987113237, 0.003478339290767908, 0.0033523044963926075], 'eval_losses': [2.164647883682251, 1.6545580722427369, 1.4628006410598755, 1.1600808632278443, 1.3669910679244994, 0.9744308032226563, 0.9985012427902221, 0.9576312366867066, 0.8076681733703613, 0.8454236519622803, 0.9638540901184082, 0.7826630718803406, 1.104512394886017, 0.7641541923141479, 0.6548123612785339, 0.9922218333053588, 0.8117137370300292, 1.2596006659698487, 0.8352864991950989, 0.7102846459007263, 0.948417996635437, 0.6667930041122436, 0.6735515013122558, 0.6765720962524414, 0.6404460549545288, 0.5868661316299438, 0.8539215717697144, 0.671702494392395, 1.013946421394348, 0.7828061346626282, 0.6231124632453918, 0.9472909074783326, 0.9412059941101074, 0.7917986108016968, 0.6953180638313293, 0.5138192662715911, 0.6458280938911438, 0.5993597709274292, 0.6608743241500854, 0.5610510663795472, 0.9400737268257141, 0.9475615718841552, 0.6772247442245484, 0.7058313579368591, 0.6999283320236206, 0.7601127938270569, 0.5406990502738953, 0.6744494660568238, 0.657445820388794, 0.6360727245140075, 0.602183510017395, 0.6638840437507629, 0.5764787740898132, 0.7622425645637512, 0.6026571141052246, 0.6648900294494628, 0.8960332232666015, 0.6581889182662964, 0.6574881332015992, 0.764426356048584, 0.5661938891410828, 0.5509537802505493, 0.6939318278503418, 0.8738080065155029, 0.8335734016609192, 0.5141450018692016, 0.5745392135429382, 0.8005889149475097, 0.5406961914157867, 0.5273537969589234, 0.7863380772590637, 0.5588566858005524, 0.6890827312850952, 0.6057677677345276, 0.5635084333992004, 0.6811963825798034, 0.61648023935318, 1.012724828414917, 0.6362171352672576, 0.600728142566681, 0.8946510879516602, 0.6929350837135315, 0.7718009796142579, 0.6391929166984558, 0.5173586741065979, 0.5696404926109314, 0.696883238658905, 0.5419068127822876, 0.5304592200279236, 0.49453839639663694, 0.6299375939750671, 0.5407747402286529, 0.5964672617912292, 0.6953459352493286, 0.4981718883323669, 0.5146202296638489, 0.6263791172599793, 0.4971261043357849, 0.541742048034668, 0.5074004801368713, 0.5106167580413818, 0.5403941158676148, 0.4952197231388092, 0.5036902098464966, 0.5564737716388702, 0.6205085544586182, 0.5223774889564514, 0.636497908191681, 0.5072039118480682, 0.5414098038864136, 0.577336726436615, 0.6121881593322754, 0.5503580962467194, 0.5647501354217529, 0.5259331694030762, 0.5857357997512818, 0.6622110190010071, 0.5913814347076416, 0.5286097303771973, 0.6344818830299378, 0.5167601127433776, 0.5717268344116211, 0.503782015247345, 0.5718589818191528, 0.5040748626327515, 0.5289617172050476, 0.5618893656158447, 0.6697765984916687, 0.4896399263381958, 0.5026871321678161, 0.5995179688262939, 0.5867765543365479, 0.6022817820549011, 0.5503802780532837, 0.5891882818222046, 0.5659550923728943, 0.5393634149551392, 0.4725204941082001, 0.5599682631874084, 0.49285970909118654, 0.4587254029464722, 0.5250094151496887, 0.4481734387016296, 0.5108769823455811, 0.46357153235435483, 0.46746708969116213, 0.5047348567390442, 0.5160715837097168, 0.4970678808498383, 0.47975242698669435, 0.46656886087417604, 0.4607551993751526, 0.4550916090297699, 0.4969077893257141, 0.45320259128570556, 0.4825685316467285, 0.4614853197288513, 0.4551181153869629, 0.4619613722896576, 0.4459185376644135, 0.4371619209480286, 0.43250919277191163, 0.4586801371669769, 0.4258842087650299, 0.4228464393901825, 0.43363750972747805, 0.424913526930809, 0.42323920888900757, 0.43190860176086426, 0.4162640994262695, 0.4204883956050873, 0.42004101861953735, 0.41806100769519805, 0.41835685523986815, 0.4199794206905365, 0.4170577170944214, 0.4155419568443298, 0.4160143413925171, 0.4168384044075012, 0.4160309219741821, 0.4137410012769699, 0.4120999512767792, 0.41584254943847654, 0.4088985359096527, 0.41060246324539185, 0.4111568983078003, 0.4107550534629822, 0.415529646282196, 0.413400959815979, 0.41062481578826904, 0.41101011686325073, 0.4162301109313965, 0.41249493049621583, 0.411628330783844, 0.4098601459789276, 0.41143245264053346, 0.41225017765045163, 0.413348842458725, 0.41110972929000855, 0.41610408491134643, 0.4268304099082947], 'train_acc1es': [33.06400000732422, 53.84400000122071, 62.50399999267578, 67.13199997314453, 70.876, 73.9, 76.27199998535156, 77.71199999755859, 79.13600000976562, 80.49200001464844, 81.03199999023437, 81.96000001953125, 82.45600001708985, 82.82399997802735, 83.49999999023437, 84.28000000244141, 84.20799997070313, 84.70000001220703, 85.17199997802734, 85.36399999023438, 85.78799997558593, 85.72800000488282, 86.34800000244141, 86.48400000488282, 86.36799999267578, 87.00000000244141, 87.13600001708984, 87.4, 87.644, 87.2879999975586, 87.82800001464844, 87.57200001708985, 87.85599999511719, 88.41599999511719, 88.52000001464843, 88.6, 88.87599998535157, 88.7480000024414, 88.6, 89.07200001708985, 88.96, 89.72399999023438, 89.648, 89.4799999975586, 89.31999997802734, 89.83199999511719, 89.99199998779297, 89.57999998046876, 90.09199999511719, 90.32799999511718, 89.94399998291016, 90.49999999755859, 90.60799999267579, 90.68799997802735, 90.72399999023438, 90.81200000732422, 90.4119999975586, 90.97999999267579, 90.916, 91.71199998046875, 90.89999998291016, 91.09199997314452, 91.5200000024414, 91.10399998291015, 91.51999998779297, 91.52399999023437, 91.75599998779298, 91.45199998779297, 91.81999997558594, 91.63199999511718, 92.27999998291016, 92.4799999975586, 92.10799997070312, 92.08799997558594, 92.76399999511719, 92.51199998779298, 92.64799997558593, 92.59199999511719, 92.53599998046874, 92.73999997558593, 92.94399999755859, 92.86399997802734, 93.09999997558593, 93.384, 93.01599997802734, 93.29999999267578, 93.23600001220703, 93.95200001464843, 93.88000001708984, 93.61599997558594, 93.92800001953125, 94.21199997558594, 94.00399997070312, 93.98399998046875, 94.18399997314454, 94.36399998046875, 94.32399999267578, 94.55599997558593, 94.58799998046875, 94.58800001708984, 94.55999997802735, 94.78800001708984, 95.09600000732422, 95.01199997070313, 95.12400001953125, 95.02399998046874, 95.06800001953125, 95.40399997070313, 95.43999997314454, 95.70799997314454, 95.76000000732422, 96.04800001220703, 95.77599997802734, 95.87600001220703, 96.02800000976562, 96.09200001220704, 96.17599997314453, 96.31600001953124, 96.32400001708984, 96.41200000976562, 97.02400000976563, 96.47600001220704, 97.00800000976562, 96.90800001464844, 97.25600000488281, 97.16400001220703, 97.16800001220703, 97.08800000488282, 97.33600000976563, 97.62000000488281, 97.27200000732422, 97.5920000048828, 97.59600000976563, 97.96800000488281, 98.12399997070312, 98.02800000488281, 98.3680000024414, 98.48000000488281, 98.34400000488282, 98.4640000048828, 98.46800001953125, 98.66000000488282, 98.73200001220704, 98.98000000488281, 98.99200000732422, 99.08400000488281, 99.21600000732421, 99.05600000732422, 99.25600000732422, 99.19600000488282, 99.3880000024414, 99.3760000024414, 99.34800000732422, 99.3800000024414, 99.45600000488281, 99.58400000732422, 99.60400000244141, 99.568, 99.704, 99.73200000244141, 99.72, 99.736, 99.768, 99.824, 99.828, 99.868, 99.912, 99.92, 99.9, 99.89600000244141, 99.9040000024414, 99.936, 99.948, 99.96, 99.968, 99.952, 99.96, 99.976, 99.956, 99.952, 99.968, 99.976, 99.988, 99.956, 99.956, 99.976, 99.972, 99.984, 99.984, 99.968, 99.964, 99.9840000024414, 99.972, 99.976, 99.988, 99.98, 99.992, 99.964, 99.948, 99.964], 'eval_acc1es': [33.344, 43.964000007324216, 54.803999992675784, 62.55600000732422, 57.4680000012207, 66.68399998046876, 64.57600001220703, 68.19200000976562, 72.40000000488281, 72.42400000732422, 70.71599999267578, 73.48400000488282, 66.98400001953125, 74.51200001953126, 78.53599998046874, 68.46400000976563, 74.052, 65.61199998535156, 73.72800000488282, 77.83199999023438, 70.62000001708985, 78.69999998779296, 78.70799997314454, 78.82799999023437, 79.01999998291015, 80.87199997314453, 75.85199998291016, 77.97600001464843, 70.88799999023438, 75.66, 80.45199999267578, 74.11199998535156, 72.39599997070313, 77.23999998779297, 78.49599997558593, 83.26400001953125, 79.86799997314453, 80.86399997314453, 79.84799997558594, 82.13999998291015, 73.98399999267578, 73.21199997070312, 80.00399998046875, 79.31999999023438, 78.55999999023437, 77.32799999023437, 82.14400001464844, 79.76000001953125, 79.46400000244141, 81.29999999023437, 82.09999997558593, 80.80400001464844, 81.46400001953126, 78.54799998046875, 81.27199997558594, 80.28799999267578, 74.4479999975586, 80.75999997070312, 79.66399997314453, 79.67200000976563, 82.46000001464844, 82.94800001220703, 79.46399999267578, 75.77600001953125, 76.55199999267577, 84.17200001708984, 82.84399997558593, 78.06400000732422, 83.29600001708984, 83.88400001708985, 78.0999999975586, 83.05200001464844, 80.34799997802735, 81.83599998779297, 83.07200001953125, 80.64800001953125, 81.63599998046875, 73.6560000024414, 81.51600001708984, 82.86799997314453, 76.00400000488281, 80.75200001953125, 78.76799999023437, 81.66799997314453, 85.23600000488281, 83.28000000244141, 80.49600001953125, 84.19600001464843, 84.69199997314453, 85.31199997070313, 81.82000001953125, 84.21600001464844, 83.71600001953125, 82.01599997802734, 85.52800000976562, 85.24399997314453, 82.19999997314453, 85.89200000488282, 84.30400001708985, 85.43999997802734, 84.89200001708984, 84.336, 85.76800000244141, 85.12800001708985, 84.37600001220703, 82.62800001464844, 85.66399999755859, 82.81599997802735, 85.33999999755859, 85.16799997070312, 84.60400001464843, 83.93600000732422, 84.82, 85.06800001708984, 85.49600001464843, 84.53200001220704, 82.67200001953125, 84.89600001708985, 85.56400000488281, 83.27999997802735, 85.84800001953126, 85.16400000488281, 86.22400001464844, 85.64800001953125, 87.1240000024414, 85.80799998046875, 84.71599999511719, 83.96000000488282, 87.04799999023437, 87.37999999267578, 84.604, 85.54000000488281, 85.18399997314454, 85.95200000976563, 85.15999997070313, 85.824, 86.804, 88.17199999267578, 86.368, 87.824, 88.35999999511719, 86.96399997070313, 88.5159999975586, 87.6240000024414, 88.36799999267578, 88.30399997802735, 87.97200000976562, 87.88800001708984, 88.056, 88.57599998779297, 88.68000000976562, 88.64799999023438, 89.184, 88.39599999267578, 89.404, 88.8519999975586, 89.16799998291016, 89.32399999023437, 89.30799998291016, 89.74399999023437, 89.86, 89.80799999511719, 89.1800000024414, 90.05999998535157, 89.9839999975586, 89.90399997314454, 90.14799997558593, 90.14799998779297, 90.0959999975586, 90.15999999267578, 90.16799998535156, 90.41599998535156, 90.28799997314454, 90.27599999511719, 90.31999998046875, 90.45599999023437, 90.37599998535157, 90.30799998779297, 90.49999997802735, 90.35199997558594, 90.33600001708984, 90.42799999023437, 90.31999998779297, 90.43999998535156, 90.34399998779297, 90.39599998779296, 90.44399999267578, 90.3839999951172, 90.38799999023438, 90.35599999267578, 90.43999998291015, 90.30399997802735, 90.36399998291016, 90.38799999023438, 90.45199999023437, 90.35599999511719, 90.35999999023437, 90.36000001708985, 90.42799998046875, 90.2959999975586, 90.21], 'cost_info': {'flops': 54.96897, 'params': 0.400346, 'latency': 0.02005649867810701, 'train_time': 10.839458545049032}}}\n",
            "\u001b[32m[05/15 12:54:08 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:08 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:08 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:08 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:08 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 23\n",
            "\u001b[32m[05/15 12:54:08 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:08 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:08 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:08 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:09 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:09 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:09 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:09 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:09 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:09 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:09 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:09 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:10 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 29\n",
            "\u001b[32m[05/15 12:54:10 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:10 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[05/15 12:54:10 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:10 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:10 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:11 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:11 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:11 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy (top1, top5): 99.96400, 0.00000, Validation accuracy: 90.21000, 0.00000\n",
            "\u001b[32m[05/15 12:54:11 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:11 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:11 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:12 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:12 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:13 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:13 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:13 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:13 nl.defaults.trainer]: \u001b[0mEpoch 43, Anytime results: {'cifar10-valid': {'train_losses': [1.7569053060531616, 1.2664795138931275, 1.0398911100578307, 0.9061412560462951, 0.8145561787033081, 0.7282953679084778, 0.6751605457687377, 0.6325034687423706, 0.594797575340271, 0.5714447735023499, 0.5462822461795807, 0.5183249447441101, 0.5060108277034759, 0.4860454574871063, 0.47330892276763914, 0.45837394062042236, 0.4454547724342346, 0.4367031452178955, 0.4227136728191376, 0.4114862308216095, 0.4085151299095154, 0.4044977221775055, 0.38876685503959657, 0.38440722067832944, 0.3794570953273773, 0.3773342935180664, 0.36700300178527834, 0.370111882314682, 0.35280171340942384, 0.3534411968040466, 0.34880874015808105, 0.3392093047142029, 0.33516052363395693, 0.33190925087928774, 0.3343107649898529, 0.3237089006614685, 0.3215712014198303, 0.3171178406906128, 0.30574725152015686, 0.31025762138366697, 0.30747290229797364, 0.29834707723617554, 0.29427569427490236, 0.29449756768226626, 0.29159496780872346, 0.29581981219291686, 0.2777541013431549, 0.28210052105903627, 0.2820353820800781, 0.2740921457147598, 0.2777707223701477, 0.27363268907546995, 0.27083577856063845, 0.2679726627254486, 0.25681533714294436, 0.2701511300086975, 0.2552899360322952, 0.25207480679512023, 0.2520826415157318, 0.25758047731399536, 0.23979794030189513, 0.24494134342193605, 0.2434393112182617, 0.23536221855163575, 0.22918426724910737, 0.23721468706130983, 0.22874721122741698, 0.22816264547348022, 0.2232528401231766, 0.22619380402088166, 0.2239677005815506, 0.21510418560028077, 0.2157858929157257, 0.2110674531555176, 0.2087889788532257, 0.21243365513324738, 0.20813079245567323, 0.20389643756866455, 0.19675868477344513, 0.19769202645778655, 0.19059690853595734, 0.19251479341506958, 0.18629339147090912, 0.19223606120109557, 0.18190774082660674, 0.19243542908668518, 0.1761055871105194, 0.17961711973190309, 0.17529628326892852, 0.17030297406196596, 0.16327535594940185, 0.1726509228181839, 0.163730563454628, 0.16315957696914674, 0.15815423311471938, 0.15503937411785126, 0.16293003674030304, 0.15117002528190612, 0.13990541839122772, 0.15702265500068666, 0.1439029846382141, 0.14200549327373505, 0.13383040287971495, 0.1406310504436493, 0.1364821756696701, 0.1288030527639389, 0.1294026828765869, 0.12637779561519621, 0.12131864381313325, 0.125661949262619, 0.12044794710159301, 0.11840331094264984, 0.11663053024053574, 0.11261456810951233, 0.11047457273960114, 0.1045703645825386, 0.1094984476852417, 0.10084918767690658, 0.10446519842624664, 0.09294993334293365, 0.09034513345956803, 0.08801165543079376, 0.08451360163211823, 0.07844458384990692, 0.08243906379699707, 0.08179043726205826, 0.07237742782592774, 0.07404884174466134, 0.07023863109588623, 0.06873067468523979, 0.07019079623937607, 0.06733777047038078, 0.06257209670066834, 0.0591178988468647, 0.053110786118507385, 0.057006341593265536, 0.054067337636947634, 0.048774153035879134, 0.04665558832645416, 0.05043800618171692, 0.042935804923176765, 0.03886122808098793, 0.03610275668859482, 0.03468259646952152, 0.029686471796035768, 0.03174582464873791, 0.029260967531204225, 0.023907402811050416, 0.026981608538627626, 0.0215273837184906, 0.022613820883035658, 0.02012240275144577, 0.016215642614364624, 0.017552341111302376, 0.016573177622407675, 0.014351837279200554, 0.015186065632998944, 0.013720216526985168, 0.012305273621082306, 0.009820013044774532, 0.009034747025966645, 0.008558783235102892, 0.008558888110518456, 0.007024007311463356, 0.006049726008176804, 0.006208703562021255, 0.0055816052811592815, 0.005483860322833062, 0.005120645933151245, 0.004940851823240519, 0.004379023199342191, 0.004500048578828573, 0.003829212036281824, 0.0034377170379459856, 0.003725832347124815, 0.0036780160421878098, 0.003731304369196296, 0.0033510872841253877, 0.00363513479270041, 0.00350781282491982, 0.0033855385491251945, 0.0032582418429851533, 0.0029350116714835168, 0.003581193818822503, 0.0029704603014886377, 0.0025494210241734983, 0.003201259375065565, 0.0027124761680141093, 0.0028953837013989685, 0.0030145811362564563, 0.0028955158534646032, 0.0027816221611201763, 0.0028242863665521147, 0.0024912390230596064, 0.0025807611463963985, 0.0028283434772491457, 0.0026745883461833002, 0.002818832339346409, 0.0026991494470089672, 0.002995031623542309], 'eval_losses': [1.5692320011138916, 1.692255509376526, 1.259640083770752, 1.0358222034072877, 1.1081641393280028, 0.898577421207428, 0.8860601475715637, 0.7382295448493957, 0.813926875, 0.888411948184967, 0.7486177971839905, 0.7528334671783448, 0.9931170081329346, 0.8498606488990784, 0.6512779799270629, 0.7387924125480652, 0.8116760023117066, 0.7299716124153137, 0.6730028191947937, 0.6996099607467652, 0.6411852591514587, 0.9248606335830688, 0.6231177605438233, 0.890373495979309, 0.5836540268135071, 0.6635516252326965, 0.9188884570503235, 0.6369978741645813, 0.8734158744049072, 0.7266159543991089, 0.6872789871215821, 0.813729638004303, 0.6383802032470703, 0.7549205728912354, 0.6218803883171081, 0.700077579498291, 0.6347955551338196, 0.6381415802383423, 0.7855197314834594, 0.6381312977790833, 0.6686823948478698, 0.7025910236167908, 0.6351864266395569, 0.6211878901100159, 0.61226732421875, 0.7281109514617919, 0.6917098893928528, 0.816404071598053, 0.6148451482582092, 0.7654599810791015, 0.6374987333869934, 0.6595930378913879, 0.5458997198104858, 0.7432138886642456, 0.5766457987594604, 0.6001726524925232, 0.7027325080490112, 0.7266061331176757, 0.6104864811897278, 0.5152941138839722, 0.5384895776557922, 0.4726872467136383, 0.5839475108337402, 0.6504479524230957, 0.6767966947555542, 0.8578970531463623, 0.5744328165435791, 0.5219236636161804, 0.7383748282051087, 0.48225053327560424, 0.634156963005066, 0.5497831882286072, 0.6862307289505005, 0.9092927713394166, 0.5589307807159424, 0.6638886155891418, 0.733991358089447, 0.555501425113678, 0.6120970909118653, 0.516873232345581, 0.8179938382148743, 0.5805805414962768, 0.6991592945671081, 0.6539873460960388, 0.5709471417427063, 0.6375534070968628, 0.5402056052017212, 0.6883054086875916, 0.5828042843151092, 0.6054750591659546, 0.566019707660675, 0.9152434265327454, 0.4966023212528229, 0.6784646076965332, 0.5414937542724609, 0.49248072995185854, 0.5574530310726166, 0.6342940159797669, 0.624105606880188, 0.754045444278717, 0.5569883807754517, 0.469898601732254, 0.5703178303909302, 0.5973637752342225, 0.5329736370658874, 0.5648937179946899, 0.5659561194038392, 0.5896530161857605, 0.5579477121353149, 0.6534712743759156, 0.5268841871261597, 0.6428838219451904, 0.6437939745140075, 0.48700583701133726, 0.5503930351448059, 0.6020848330116272, 0.4981022983932495, 0.46053907823562623, 0.4978150617599487, 0.6112286166381836, 0.4818751058101654, 0.4703215942335129, 0.5539687369918823, 0.5854696180915833, 0.5198580288219452, 0.5484057223129273, 0.6158625351619721, 0.5792768238639832, 0.5337661961746216, 0.5886052313423157, 0.54467154712677, 0.473427551279068, 0.4816957168865204, 0.5139731049251557, 0.550352405872345, 0.4698507158279419, 0.5226176941871643, 0.48719725017547605, 0.48560441466331483, 0.4822244606876373, 0.5956338468170166, 0.5404972162437439, 0.46963005029678345, 0.5527439107131958, 0.513271921749115, 0.4574300882911682, 0.4538339582061768, 0.47028755069732664, 0.42532012575149536, 0.4283977487182617, 0.4530713115692139, 0.4484299855709076, 0.46192191455841064, 0.4623718216705322, 0.47747744750022886, 0.4378578797149658, 0.44730199337005616, 0.4541953098487854, 0.4522034854507446, 0.4571830992126465, 0.4335714791870117, 0.427637976436615, 0.4394387770175934, 0.4353127297210693, 0.4287326905155182, 0.4174875314235687, 0.4283384783363342, 0.4249683612442017, 0.41909786179065706, 0.42239480409622193, 0.4236892115879059, 0.42108781467437745, 0.4160034220790863, 0.41670239393234254, 0.41843662134170534, 0.417412882938385, 0.4146304425907135, 0.420097362241745, 0.4165667124938965, 0.4135247115325928, 0.41516472861289977, 0.4136860141944885, 0.41217745824813845, 0.4159237167835236, 0.4151900646591187, 0.40994485832214356, 0.41669623952865603, 0.41205567766189577, 0.412605232963562, 0.41245238943099977, 0.41567705671310423, 0.41590032676696775, 0.41446701001167296, 0.4119553282356262, 0.412061138677597, 0.41358045135498045, 0.4162572134208679, 0.41225020356655123, 0.4152240486907959, 0.4137391041278839, 0.4160864718437195], 'train_acc1es': [32.86799998901367, 53.528000007324216, 62.39199997558594, 67.29599998291016, 70.92000001464844, 74.30399999511718, 76.35999998046876, 77.848, 79.05599998046875, 80.1439999975586, 81.26000001464844, 82.17599997314453, 82.22800001953125, 83.21200000488281, 83.74799997314453, 84.37599997314453, 84.82800001464844, 84.876, 85.21999999755859, 85.7, 85.77600000976562, 85.996, 86.67199998779297, 86.66000000244141, 86.87199999023437, 86.84000000488281, 87.42, 87.04400001708984, 87.7240000024414, 87.8480000048828, 87.74400000244141, 87.99199999511718, 88.35999999023437, 88.55199999511719, 88.55599998291015, 88.90799999755859, 88.88800000976562, 89.06000000488281, 89.30399998535157, 89.13599999511719, 89.29199999267578, 89.60800000488281, 89.7960000024414, 89.87599999511718, 89.94799998535156, 89.63200000976562, 90.468, 90.25999999511718, 90.17599998535157, 90.48399998046875, 90.256, 90.44799997070312, 90.67999998535156, 90.6239999975586, 90.95999999511719, 90.47199998779297, 91.05199999023438, 91.09199997802735, 91.31599998291016, 90.99999998535156, 91.65999999511719, 91.24399998046874, 91.51199998535157, 91.91599999267578, 92.03999997558594, 91.8519999975586, 92.0359999951172, 92.12000000488281, 92.23599998291016, 92.03199997558593, 92.15999997070313, 92.61599998779297, 92.39999999267579, 92.65599999023438, 92.66399997802735, 92.43599998779297, 92.73199998291015, 92.84399999267578, 92.98399998046875, 93.02399998291015, 93.29599997558594, 93.41199997802734, 93.61199997314453, 93.29599999023438, 93.79199998779296, 93.40799998046874, 94.04799997070313, 93.87199997314453, 93.85599998046875, 94.07199997314453, 94.29999997802734, 94.05999997558594, 94.37600001708985, 94.50799997314454, 94.46400000976563, 94.61200001464844, 94.27999997558594, 94.87199997070313, 95.18399997314454, 94.59999999023438, 95.19599997070313, 95.13599997314454, 95.35199997314453, 95.17599997558594, 95.31999997070312, 95.41199998291016, 95.57199998046875, 95.63600001464843, 95.59200001220704, 95.77999998046874, 95.89200001953125, 95.85599997314453, 95.98400001953125, 96.16800001464844, 96.29600001953125, 96.37200001220702, 96.30799997314453, 96.54400001464843, 96.46000001220703, 96.76799997070313, 97.09200001464843, 97.06399997558594, 97.15600001464844, 97.43200001464844, 97.29599997070312, 97.24400001464844, 97.59199997314452, 97.59600000732422, 97.64000000976563, 97.7280000024414, 97.64400001464844, 97.79200000732422, 97.98000001220703, 98.0720000024414, 98.28400001708984, 98.12800000244141, 98.21600000976562, 98.49200000976562, 98.50400001220703, 98.41600000976563, 98.56800000488282, 98.8600000048828, 98.85200000488281, 98.98800000244141, 99.08400000488281, 99.05600000244141, 99.12800000244141, 99.29600000488281, 99.18800000488281, 99.38800000732422, 99.39200000488282, 99.42000000488281, 99.60800000732422, 99.5080000024414, 99.536, 99.65200000488281, 99.592, 99.6560000024414, 99.732, 99.796, 99.7760000024414, 99.82, 99.816, 99.884, 99.9120000024414, 99.90800000244141, 99.892, 99.9000000024414, 99.928, 99.916, 99.948, 99.924, 99.972, 99.968, 99.964, 99.948, 99.964, 99.968, 99.952, 99.948, 99.96, 99.96, 99.984, 99.948, 99.984, 99.992, 99.964, 99.992, 99.964, 99.972, 99.976, 99.98, 99.984, 100.0, 99.988, 99.984, 99.984, 99.976, 99.976, 99.968], 'eval_acc1es': [41.03999999633789, 45.6880000012207, 57.227999998779296, 63.77199999023438, 62.38399999267578, 69.82399997558593, 70.60799998779297, 74.79199998046874, 72.80800001708984, 71.01599999267579, 75.71999997314452, 75.15999998046875, 66.86, 73.90399999511719, 78.76, 75.73600001708985, 75.01999998779297, 75.94400000976563, 77.948, 77.128, 78.49600001708984, 72.59999999023438, 80.14000001953124, 73.75600000976563, 81.30800001464844, 79.14799998779297, 73.75999998779297, 79.24399997802735, 74.55199998291016, 77.89599998779296, 78.79199999267578, 76.60399999023437, 80.02399997802735, 77.98399999511719, 80.54399998046875, 79.12399997070312, 80.11199997314453, 80.28399998779297, 77.16000000976562, 80.31199998535156, 79.8320000024414, 77.94800001464844, 80.63600001953125, 81.14799997802734, 80.65599997558594, 78.70799999511719, 78.72399998046875, 76.936, 81.69599998291015, 77.57199997558594, 81.30400001708985, 79.71199998779296, 83.00000000976563, 77.47599997070313, 82.50400001708984, 81.89600000732422, 79.752, 79.09599998046875, 81.71599998046875, 83.59599997070312, 83.55599997558593, 85.29599999267577, 82.30799997070312, 80.79200001953124, 81.04400001708984, 76.46000000976562, 82.97600000732422, 83.59599997802735, 78.57199999511718, 84.72800001220703, 80.96799999267579, 83.05199997070312, 80.77600001708984, 76.216, 82.832, 80.96800001953125, 80.05599998779297, 83.77200001220703, 81.68400001953125, 84.48800001953126, 77.98399998291016, 83.22799998535156, 80.58000000732422, 81.33599997802735, 83.29599999267577, 81.66000000244141, 83.95999998046875, 80.91999997070313, 82.90400001220704, 82.87600001220703, 83.94000000488282, 76.33599997802735, 85.28800000976563, 82.06400001708984, 84.24800001708985, 85.172, 84.23200000732422, 82.84000001708985, 82.76000001464844, 81.17599998535157, 84.16000000976562, 86.372, 84.9600000024414, 84.12800000732422, 85.32800001220703, 84.59600001708985, 84.35600000488282, 83.8560000024414, 85.38000000488282, 82.80400000732422, 85.43200001953124, 83.46800001708985, 82.72400001708985, 86.5519999975586, 85.53200000976562, 84.38800000976562, 86.41200000732422, 87.2719999975586, 86.27199997070312, 84.31200000488282, 87.14799998535156, 87.47599998779297, 86.25200000488282, 85.28400001220703, 86.26399999511719, 85.78000000976563, 84.824, 85.95600001220703, 86.45600000976563, 85.24800000732422, 86.38800000488281, 87.76399998046875, 87.66399999267578, 87.2159999975586, 86.19599999755859, 88.30799998291016, 87.296, 87.94799997802734, 88.15599998779297, 88.10799999267579, 86.59999999023438, 87.43599999023438, 88.64800001953125, 87.35200001464844, 87.81599999267578, 89.0279999975586, 89.04400000244141, 88.91199998291016, 89.42000000244141, 89.656, 89.24799999511718, 89.55599997802734, 89.372, 89.47599999511719, 89.17199998291015, 89.66400001220703, 89.89199999511719, 89.48799999267578, 89.65199999755859, 89.83199998779297, 90.0359999951172, 90.30799998779297, 89.94800000732423, 90.18799999755859, 90.49599998535156, 90.62399997070312, 90.38799998779297, 90.52000001220703, 90.73199998046876, 90.52799997558594, 90.53999998779297, 90.57599998779297, 90.80000001953125, 90.69999998291016, 90.7639999975586, 90.70399998046875, 90.82399999267578, 90.71599997070312, 90.76800000244141, 90.77599998291015, 90.85599998291016, 90.77599998291015, 90.75999998291016, 90.77999998535157, 90.83199998535156, 90.79599998046875, 90.80399999267578, 90.77599999023437, 90.73599999511718, 90.79199999023437, 90.78400001220703, 90.8, 90.76799998291015, 90.80399999023437, 90.81999998535156, 90.81999998535156, 90.78799999267578, 90.84400001708984, 90.78799999511719, 90.79199997070313, 90.8], 'cost_info': {'flops': 86.42625, 'params': 0.615386, 'latency': 0.019886672496795654, 'train_time': 11.101390759150187}}}\n",
            "\u001b[32m[05/15 12:54:13 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:13 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:13 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:14 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:14 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:14 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:14 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:14 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:15 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:15 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:16 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:16 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:16 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:16 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:16 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:16 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:17 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:17 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:17 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:17 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:17 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:18 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:18 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:18 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:19 nl.defaults.trainer]: \u001b[0mEpoch 67, Anytime results: {'cifar10-valid': {'train_losses': [1.7569053060531616, 1.2664795138931275, 1.0398911100578307, 0.9061412560462951, 0.8145561787033081, 0.7282953679084778, 0.6751605457687377, 0.6325034687423706, 0.594797575340271, 0.5714447735023499, 0.5462822461795807, 0.5183249447441101, 0.5060108277034759, 0.4860454574871063, 0.47330892276763914, 0.45837394062042236, 0.4454547724342346, 0.4367031452178955, 0.4227136728191376, 0.4114862308216095, 0.4085151299095154, 0.4044977221775055, 0.38876685503959657, 0.38440722067832944, 0.3794570953273773, 0.3773342935180664, 0.36700300178527834, 0.370111882314682, 0.35280171340942384, 0.3534411968040466, 0.34880874015808105, 0.3392093047142029, 0.33516052363395693, 0.33190925087928774, 0.3343107649898529, 0.3237089006614685, 0.3215712014198303, 0.3171178406906128, 0.30574725152015686, 0.31025762138366697, 0.30747290229797364, 0.29834707723617554, 0.29427569427490236, 0.29449756768226626, 0.29159496780872346, 0.29581981219291686, 0.2777541013431549, 0.28210052105903627, 0.2820353820800781, 0.2740921457147598, 0.2777707223701477, 0.27363268907546995, 0.27083577856063845, 0.2679726627254486, 0.25681533714294436, 0.2701511300086975, 0.2552899360322952, 0.25207480679512023, 0.2520826415157318, 0.25758047731399536, 0.23979794030189513, 0.24494134342193605, 0.2434393112182617, 0.23536221855163575, 0.22918426724910737, 0.23721468706130983, 0.22874721122741698, 0.22816264547348022, 0.2232528401231766, 0.22619380402088166, 0.2239677005815506, 0.21510418560028077, 0.2157858929157257, 0.2110674531555176, 0.2087889788532257, 0.21243365513324738, 0.20813079245567323, 0.20389643756866455, 0.19675868477344513, 0.19769202645778655, 0.19059690853595734, 0.19251479341506958, 0.18629339147090912, 0.19223606120109557, 0.18190774082660674, 0.19243542908668518, 0.1761055871105194, 0.17961711973190309, 0.17529628326892852, 0.17030297406196596, 0.16327535594940185, 0.1726509228181839, 0.163730563454628, 0.16315957696914674, 0.15815423311471938, 0.15503937411785126, 0.16293003674030304, 0.15117002528190612, 0.13990541839122772, 0.15702265500068666, 0.1439029846382141, 0.14200549327373505, 0.13383040287971495, 0.1406310504436493, 0.1364821756696701, 0.1288030527639389, 0.1294026828765869, 0.12637779561519621, 0.12131864381313325, 0.125661949262619, 0.12044794710159301, 0.11840331094264984, 0.11663053024053574, 0.11261456810951233, 0.11047457273960114, 0.1045703645825386, 0.1094984476852417, 0.10084918767690658, 0.10446519842624664, 0.09294993334293365, 0.09034513345956803, 0.08801165543079376, 0.08451360163211823, 0.07844458384990692, 0.08243906379699707, 0.08179043726205826, 0.07237742782592774, 0.07404884174466134, 0.07023863109588623, 0.06873067468523979, 0.07019079623937607, 0.06733777047038078, 0.06257209670066834, 0.0591178988468647, 0.053110786118507385, 0.057006341593265536, 0.054067337636947634, 0.048774153035879134, 0.04665558832645416, 0.05043800618171692, 0.042935804923176765, 0.03886122808098793, 0.03610275668859482, 0.03468259646952152, 0.029686471796035768, 0.03174582464873791, 0.029260967531204225, 0.023907402811050416, 0.026981608538627626, 0.0215273837184906, 0.022613820883035658, 0.02012240275144577, 0.016215642614364624, 0.017552341111302376, 0.016573177622407675, 0.014351837279200554, 0.015186065632998944, 0.013720216526985168, 0.012305273621082306, 0.009820013044774532, 0.009034747025966645, 0.008558783235102892, 0.008558888110518456, 0.007024007311463356, 0.006049726008176804, 0.006208703562021255, 0.0055816052811592815, 0.005483860322833062, 0.005120645933151245, 0.004940851823240519, 0.004379023199342191, 0.004500048578828573, 0.003829212036281824, 0.0034377170379459856, 0.003725832347124815, 0.0036780160421878098, 0.003731304369196296, 0.0033510872841253877, 0.00363513479270041, 0.00350781282491982, 0.0033855385491251945, 0.0032582418429851533, 0.0029350116714835168, 0.003581193818822503, 0.0029704603014886377, 0.0025494210241734983, 0.003201259375065565, 0.0027124761680141093, 0.0028953837013989685, 0.0030145811362564563, 0.0028955158534646032, 0.0027816221611201763, 0.0028242863665521147, 0.0024912390230596064, 0.0025807611463963985, 0.0028283434772491457, 0.0026745883461833002, 0.002818832339346409, 0.0026991494470089672, 0.002995031623542309], 'eval_losses': [1.5692320011138916, 1.692255509376526, 1.259640083770752, 1.0358222034072877, 1.1081641393280028, 0.898577421207428, 0.8860601475715637, 0.7382295448493957, 0.813926875, 0.888411948184967, 0.7486177971839905, 0.7528334671783448, 0.9931170081329346, 0.8498606488990784, 0.6512779799270629, 0.7387924125480652, 0.8116760023117066, 0.7299716124153137, 0.6730028191947937, 0.6996099607467652, 0.6411852591514587, 0.9248606335830688, 0.6231177605438233, 0.890373495979309, 0.5836540268135071, 0.6635516252326965, 0.9188884570503235, 0.6369978741645813, 0.8734158744049072, 0.7266159543991089, 0.6872789871215821, 0.813729638004303, 0.6383802032470703, 0.7549205728912354, 0.6218803883171081, 0.700077579498291, 0.6347955551338196, 0.6381415802383423, 0.7855197314834594, 0.6381312977790833, 0.6686823948478698, 0.7025910236167908, 0.6351864266395569, 0.6211878901100159, 0.61226732421875, 0.7281109514617919, 0.6917098893928528, 0.816404071598053, 0.6148451482582092, 0.7654599810791015, 0.6374987333869934, 0.6595930378913879, 0.5458997198104858, 0.7432138886642456, 0.5766457987594604, 0.6001726524925232, 0.7027325080490112, 0.7266061331176757, 0.6104864811897278, 0.5152941138839722, 0.5384895776557922, 0.4726872467136383, 0.5839475108337402, 0.6504479524230957, 0.6767966947555542, 0.8578970531463623, 0.5744328165435791, 0.5219236636161804, 0.7383748282051087, 0.48225053327560424, 0.634156963005066, 0.5497831882286072, 0.6862307289505005, 0.9092927713394166, 0.5589307807159424, 0.6638886155891418, 0.733991358089447, 0.555501425113678, 0.6120970909118653, 0.516873232345581, 0.8179938382148743, 0.5805805414962768, 0.6991592945671081, 0.6539873460960388, 0.5709471417427063, 0.6375534070968628, 0.5402056052017212, 0.6883054086875916, 0.5828042843151092, 0.6054750591659546, 0.566019707660675, 0.9152434265327454, 0.4966023212528229, 0.6784646076965332, 0.5414937542724609, 0.49248072995185854, 0.5574530310726166, 0.6342940159797669, 0.624105606880188, 0.754045444278717, 0.5569883807754517, 0.469898601732254, 0.5703178303909302, 0.5973637752342225, 0.5329736370658874, 0.5648937179946899, 0.5659561194038392, 0.5896530161857605, 0.5579477121353149, 0.6534712743759156, 0.5268841871261597, 0.6428838219451904, 0.6437939745140075, 0.48700583701133726, 0.5503930351448059, 0.6020848330116272, 0.4981022983932495, 0.46053907823562623, 0.4978150617599487, 0.6112286166381836, 0.4818751058101654, 0.4703215942335129, 0.5539687369918823, 0.5854696180915833, 0.5198580288219452, 0.5484057223129273, 0.6158625351619721, 0.5792768238639832, 0.5337661961746216, 0.5886052313423157, 0.54467154712677, 0.473427551279068, 0.4816957168865204, 0.5139731049251557, 0.550352405872345, 0.4698507158279419, 0.5226176941871643, 0.48719725017547605, 0.48560441466331483, 0.4822244606876373, 0.5956338468170166, 0.5404972162437439, 0.46963005029678345, 0.5527439107131958, 0.513271921749115, 0.4574300882911682, 0.4538339582061768, 0.47028755069732664, 0.42532012575149536, 0.4283977487182617, 0.4530713115692139, 0.4484299855709076, 0.46192191455841064, 0.4623718216705322, 0.47747744750022886, 0.4378578797149658, 0.44730199337005616, 0.4541953098487854, 0.4522034854507446, 0.4571830992126465, 0.4335714791870117, 0.427637976436615, 0.4394387770175934, 0.4353127297210693, 0.4287326905155182, 0.4174875314235687, 0.4283384783363342, 0.4249683612442017, 0.41909786179065706, 0.42239480409622193, 0.4236892115879059, 0.42108781467437745, 0.4160034220790863, 0.41670239393234254, 0.41843662134170534, 0.417412882938385, 0.4146304425907135, 0.420097362241745, 0.4165667124938965, 0.4135247115325928, 0.41516472861289977, 0.4136860141944885, 0.41217745824813845, 0.4159237167835236, 0.4151900646591187, 0.40994485832214356, 0.41669623952865603, 0.41205567766189577, 0.412605232963562, 0.41245238943099977, 0.41567705671310423, 0.41590032676696775, 0.41446701001167296, 0.4119553282356262, 0.412061138677597, 0.41358045135498045, 0.4162572134208679, 0.41225020356655123, 0.4152240486907959, 0.4137391041278839, 0.4160864718437195], 'train_acc1es': [32.86799998901367, 53.528000007324216, 62.39199997558594, 67.29599998291016, 70.92000001464844, 74.30399999511718, 76.35999998046876, 77.848, 79.05599998046875, 80.1439999975586, 81.26000001464844, 82.17599997314453, 82.22800001953125, 83.21200000488281, 83.74799997314453, 84.37599997314453, 84.82800001464844, 84.876, 85.21999999755859, 85.7, 85.77600000976562, 85.996, 86.67199998779297, 86.66000000244141, 86.87199999023437, 86.84000000488281, 87.42, 87.04400001708984, 87.7240000024414, 87.8480000048828, 87.74400000244141, 87.99199999511718, 88.35999999023437, 88.55199999511719, 88.55599998291015, 88.90799999755859, 88.88800000976562, 89.06000000488281, 89.30399998535157, 89.13599999511719, 89.29199999267578, 89.60800000488281, 89.7960000024414, 89.87599999511718, 89.94799998535156, 89.63200000976562, 90.468, 90.25999999511718, 90.17599998535157, 90.48399998046875, 90.256, 90.44799997070312, 90.67999998535156, 90.6239999975586, 90.95999999511719, 90.47199998779297, 91.05199999023438, 91.09199997802735, 91.31599998291016, 90.99999998535156, 91.65999999511719, 91.24399998046874, 91.51199998535157, 91.91599999267578, 92.03999997558594, 91.8519999975586, 92.0359999951172, 92.12000000488281, 92.23599998291016, 92.03199997558593, 92.15999997070313, 92.61599998779297, 92.39999999267579, 92.65599999023438, 92.66399997802735, 92.43599998779297, 92.73199998291015, 92.84399999267578, 92.98399998046875, 93.02399998291015, 93.29599997558594, 93.41199997802734, 93.61199997314453, 93.29599999023438, 93.79199998779296, 93.40799998046874, 94.04799997070313, 93.87199997314453, 93.85599998046875, 94.07199997314453, 94.29999997802734, 94.05999997558594, 94.37600001708985, 94.50799997314454, 94.46400000976563, 94.61200001464844, 94.27999997558594, 94.87199997070313, 95.18399997314454, 94.59999999023438, 95.19599997070313, 95.13599997314454, 95.35199997314453, 95.17599997558594, 95.31999997070312, 95.41199998291016, 95.57199998046875, 95.63600001464843, 95.59200001220704, 95.77999998046874, 95.89200001953125, 95.85599997314453, 95.98400001953125, 96.16800001464844, 96.29600001953125, 96.37200001220702, 96.30799997314453, 96.54400001464843, 96.46000001220703, 96.76799997070313, 97.09200001464843, 97.06399997558594, 97.15600001464844, 97.43200001464844, 97.29599997070312, 97.24400001464844, 97.59199997314452, 97.59600000732422, 97.64000000976563, 97.7280000024414, 97.64400001464844, 97.79200000732422, 97.98000001220703, 98.0720000024414, 98.28400001708984, 98.12800000244141, 98.21600000976562, 98.49200000976562, 98.50400001220703, 98.41600000976563, 98.56800000488282, 98.8600000048828, 98.85200000488281, 98.98800000244141, 99.08400000488281, 99.05600000244141, 99.12800000244141, 99.29600000488281, 99.18800000488281, 99.38800000732422, 99.39200000488282, 99.42000000488281, 99.60800000732422, 99.5080000024414, 99.536, 99.65200000488281, 99.592, 99.6560000024414, 99.732, 99.796, 99.7760000024414, 99.82, 99.816, 99.884, 99.9120000024414, 99.90800000244141, 99.892, 99.9000000024414, 99.928, 99.916, 99.948, 99.924, 99.972, 99.968, 99.964, 99.948, 99.964, 99.968, 99.952, 99.948, 99.96, 99.96, 99.984, 99.948, 99.984, 99.992, 99.964, 99.992, 99.964, 99.972, 99.976, 99.98, 99.984, 100.0, 99.988, 99.984, 99.984, 99.976, 99.976, 99.968], 'eval_acc1es': [41.03999999633789, 45.6880000012207, 57.227999998779296, 63.77199999023438, 62.38399999267578, 69.82399997558593, 70.60799998779297, 74.79199998046874, 72.80800001708984, 71.01599999267579, 75.71999997314452, 75.15999998046875, 66.86, 73.90399999511719, 78.76, 75.73600001708985, 75.01999998779297, 75.94400000976563, 77.948, 77.128, 78.49600001708984, 72.59999999023438, 80.14000001953124, 73.75600000976563, 81.30800001464844, 79.14799998779297, 73.75999998779297, 79.24399997802735, 74.55199998291016, 77.89599998779296, 78.79199999267578, 76.60399999023437, 80.02399997802735, 77.98399999511719, 80.54399998046875, 79.12399997070312, 80.11199997314453, 80.28399998779297, 77.16000000976562, 80.31199998535156, 79.8320000024414, 77.94800001464844, 80.63600001953125, 81.14799997802734, 80.65599997558594, 78.70799999511719, 78.72399998046875, 76.936, 81.69599998291015, 77.57199997558594, 81.30400001708985, 79.71199998779296, 83.00000000976563, 77.47599997070313, 82.50400001708984, 81.89600000732422, 79.752, 79.09599998046875, 81.71599998046875, 83.59599997070312, 83.55599997558593, 85.29599999267577, 82.30799997070312, 80.79200001953124, 81.04400001708984, 76.46000000976562, 82.97600000732422, 83.59599997802735, 78.57199999511718, 84.72800001220703, 80.96799999267579, 83.05199997070312, 80.77600001708984, 76.216, 82.832, 80.96800001953125, 80.05599998779297, 83.77200001220703, 81.68400001953125, 84.48800001953126, 77.98399998291016, 83.22799998535156, 80.58000000732422, 81.33599997802735, 83.29599999267577, 81.66000000244141, 83.95999998046875, 80.91999997070313, 82.90400001220704, 82.87600001220703, 83.94000000488282, 76.33599997802735, 85.28800000976563, 82.06400001708984, 84.24800001708985, 85.172, 84.23200000732422, 82.84000001708985, 82.76000001464844, 81.17599998535157, 84.16000000976562, 86.372, 84.9600000024414, 84.12800000732422, 85.32800001220703, 84.59600001708985, 84.35600000488282, 83.8560000024414, 85.38000000488282, 82.80400000732422, 85.43200001953124, 83.46800001708985, 82.72400001708985, 86.5519999975586, 85.53200000976562, 84.38800000976562, 86.41200000732422, 87.2719999975586, 86.27199997070312, 84.31200000488282, 87.14799998535156, 87.47599998779297, 86.25200000488282, 85.28400001220703, 86.26399999511719, 85.78000000976563, 84.824, 85.95600001220703, 86.45600000976563, 85.24800000732422, 86.38800000488281, 87.76399998046875, 87.66399999267578, 87.2159999975586, 86.19599999755859, 88.30799998291016, 87.296, 87.94799997802734, 88.15599998779297, 88.10799999267579, 86.59999999023438, 87.43599999023438, 88.64800001953125, 87.35200001464844, 87.81599999267578, 89.0279999975586, 89.04400000244141, 88.91199998291016, 89.42000000244141, 89.656, 89.24799999511718, 89.55599997802734, 89.372, 89.47599999511719, 89.17199998291015, 89.66400001220703, 89.89199999511719, 89.48799999267578, 89.65199999755859, 89.83199998779297, 90.0359999951172, 90.30799998779297, 89.94800000732423, 90.18799999755859, 90.49599998535156, 90.62399997070312, 90.38799998779297, 90.52000001220703, 90.73199998046876, 90.52799997558594, 90.53999998779297, 90.57599998779297, 90.80000001953125, 90.69999998291016, 90.7639999975586, 90.70399998046875, 90.82399999267578, 90.71599997070312, 90.76800000244141, 90.77599998291015, 90.85599998291016, 90.77599998291015, 90.75999998291016, 90.77999998535157, 90.83199998535156, 90.79599998046875, 90.80399999267578, 90.77599999023437, 90.73599999511718, 90.79199999023437, 90.78400001220703, 90.8, 90.76799998291015, 90.80399999023437, 90.81999998535156, 90.81999998535156, 90.78799999267578, 90.84400001708984, 90.78799999511719, 90.79199997070313, 90.8], 'cost_info': {'flops': 86.42625, 'params': 0.615386, 'latency': 0.019886672496795654, 'train_time': 11.101390759150187}}}\n",
            "\u001b[32m[05/15 12:54:19 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:19 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:19 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:19 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:20 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:20 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:20 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:20 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:20 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:21 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:21 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:21 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:21 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:21 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:21 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:22 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:22 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:22 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:23 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:23 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:23 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:24 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:24 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:24 nl.defaults.trainer]: \u001b[0mEpoch 90, Anytime results: {'cifar10-valid': {'train_losses': [1.7569053060531616, 1.2664795138931275, 1.0398911100578307, 0.9061412560462951, 0.8145561787033081, 0.7282953679084778, 0.6751605457687377, 0.6325034687423706, 0.594797575340271, 0.5714447735023499, 0.5462822461795807, 0.5183249447441101, 0.5060108277034759, 0.4860454574871063, 0.47330892276763914, 0.45837394062042236, 0.4454547724342346, 0.4367031452178955, 0.4227136728191376, 0.4114862308216095, 0.4085151299095154, 0.4044977221775055, 0.38876685503959657, 0.38440722067832944, 0.3794570953273773, 0.3773342935180664, 0.36700300178527834, 0.370111882314682, 0.35280171340942384, 0.3534411968040466, 0.34880874015808105, 0.3392093047142029, 0.33516052363395693, 0.33190925087928774, 0.3343107649898529, 0.3237089006614685, 0.3215712014198303, 0.3171178406906128, 0.30574725152015686, 0.31025762138366697, 0.30747290229797364, 0.29834707723617554, 0.29427569427490236, 0.29449756768226626, 0.29159496780872346, 0.29581981219291686, 0.2777541013431549, 0.28210052105903627, 0.2820353820800781, 0.2740921457147598, 0.2777707223701477, 0.27363268907546995, 0.27083577856063845, 0.2679726627254486, 0.25681533714294436, 0.2701511300086975, 0.2552899360322952, 0.25207480679512023, 0.2520826415157318, 0.25758047731399536, 0.23979794030189513, 0.24494134342193605, 0.2434393112182617, 0.23536221855163575, 0.22918426724910737, 0.23721468706130983, 0.22874721122741698, 0.22816264547348022, 0.2232528401231766, 0.22619380402088166, 0.2239677005815506, 0.21510418560028077, 0.2157858929157257, 0.2110674531555176, 0.2087889788532257, 0.21243365513324738, 0.20813079245567323, 0.20389643756866455, 0.19675868477344513, 0.19769202645778655, 0.19059690853595734, 0.19251479341506958, 0.18629339147090912, 0.19223606120109557, 0.18190774082660674, 0.19243542908668518, 0.1761055871105194, 0.17961711973190309, 0.17529628326892852, 0.17030297406196596, 0.16327535594940185, 0.1726509228181839, 0.163730563454628, 0.16315957696914674, 0.15815423311471938, 0.15503937411785126, 0.16293003674030304, 0.15117002528190612, 0.13990541839122772, 0.15702265500068666, 0.1439029846382141, 0.14200549327373505, 0.13383040287971495, 0.1406310504436493, 0.1364821756696701, 0.1288030527639389, 0.1294026828765869, 0.12637779561519621, 0.12131864381313325, 0.125661949262619, 0.12044794710159301, 0.11840331094264984, 0.11663053024053574, 0.11261456810951233, 0.11047457273960114, 0.1045703645825386, 0.1094984476852417, 0.10084918767690658, 0.10446519842624664, 0.09294993334293365, 0.09034513345956803, 0.08801165543079376, 0.08451360163211823, 0.07844458384990692, 0.08243906379699707, 0.08179043726205826, 0.07237742782592774, 0.07404884174466134, 0.07023863109588623, 0.06873067468523979, 0.07019079623937607, 0.06733777047038078, 0.06257209670066834, 0.0591178988468647, 0.053110786118507385, 0.057006341593265536, 0.054067337636947634, 0.048774153035879134, 0.04665558832645416, 0.05043800618171692, 0.042935804923176765, 0.03886122808098793, 0.03610275668859482, 0.03468259646952152, 0.029686471796035768, 0.03174582464873791, 0.029260967531204225, 0.023907402811050416, 0.026981608538627626, 0.0215273837184906, 0.022613820883035658, 0.02012240275144577, 0.016215642614364624, 0.017552341111302376, 0.016573177622407675, 0.014351837279200554, 0.015186065632998944, 0.013720216526985168, 0.012305273621082306, 0.009820013044774532, 0.009034747025966645, 0.008558783235102892, 0.008558888110518456, 0.007024007311463356, 0.006049726008176804, 0.006208703562021255, 0.0055816052811592815, 0.005483860322833062, 0.005120645933151245, 0.004940851823240519, 0.004379023199342191, 0.004500048578828573, 0.003829212036281824, 0.0034377170379459856, 0.003725832347124815, 0.0036780160421878098, 0.003731304369196296, 0.0033510872841253877, 0.00363513479270041, 0.00350781282491982, 0.0033855385491251945, 0.0032582418429851533, 0.0029350116714835168, 0.003581193818822503, 0.0029704603014886377, 0.0025494210241734983, 0.003201259375065565, 0.0027124761680141093, 0.0028953837013989685, 0.0030145811362564563, 0.0028955158534646032, 0.0027816221611201763, 0.0028242863665521147, 0.0024912390230596064, 0.0025807611463963985, 0.0028283434772491457, 0.0026745883461833002, 0.002818832339346409, 0.0026991494470089672, 0.002995031623542309], 'eval_losses': [1.5692320011138916, 1.692255509376526, 1.259640083770752, 1.0358222034072877, 1.1081641393280028, 0.898577421207428, 0.8860601475715637, 0.7382295448493957, 0.813926875, 0.888411948184967, 0.7486177971839905, 0.7528334671783448, 0.9931170081329346, 0.8498606488990784, 0.6512779799270629, 0.7387924125480652, 0.8116760023117066, 0.7299716124153137, 0.6730028191947937, 0.6996099607467652, 0.6411852591514587, 0.9248606335830688, 0.6231177605438233, 0.890373495979309, 0.5836540268135071, 0.6635516252326965, 0.9188884570503235, 0.6369978741645813, 0.8734158744049072, 0.7266159543991089, 0.6872789871215821, 0.813729638004303, 0.6383802032470703, 0.7549205728912354, 0.6218803883171081, 0.700077579498291, 0.6347955551338196, 0.6381415802383423, 0.7855197314834594, 0.6381312977790833, 0.6686823948478698, 0.7025910236167908, 0.6351864266395569, 0.6211878901100159, 0.61226732421875, 0.7281109514617919, 0.6917098893928528, 0.816404071598053, 0.6148451482582092, 0.7654599810791015, 0.6374987333869934, 0.6595930378913879, 0.5458997198104858, 0.7432138886642456, 0.5766457987594604, 0.6001726524925232, 0.7027325080490112, 0.7266061331176757, 0.6104864811897278, 0.5152941138839722, 0.5384895776557922, 0.4726872467136383, 0.5839475108337402, 0.6504479524230957, 0.6767966947555542, 0.8578970531463623, 0.5744328165435791, 0.5219236636161804, 0.7383748282051087, 0.48225053327560424, 0.634156963005066, 0.5497831882286072, 0.6862307289505005, 0.9092927713394166, 0.5589307807159424, 0.6638886155891418, 0.733991358089447, 0.555501425113678, 0.6120970909118653, 0.516873232345581, 0.8179938382148743, 0.5805805414962768, 0.6991592945671081, 0.6539873460960388, 0.5709471417427063, 0.6375534070968628, 0.5402056052017212, 0.6883054086875916, 0.5828042843151092, 0.6054750591659546, 0.566019707660675, 0.9152434265327454, 0.4966023212528229, 0.6784646076965332, 0.5414937542724609, 0.49248072995185854, 0.5574530310726166, 0.6342940159797669, 0.624105606880188, 0.754045444278717, 0.5569883807754517, 0.469898601732254, 0.5703178303909302, 0.5973637752342225, 0.5329736370658874, 0.5648937179946899, 0.5659561194038392, 0.5896530161857605, 0.5579477121353149, 0.6534712743759156, 0.5268841871261597, 0.6428838219451904, 0.6437939745140075, 0.48700583701133726, 0.5503930351448059, 0.6020848330116272, 0.4981022983932495, 0.46053907823562623, 0.4978150617599487, 0.6112286166381836, 0.4818751058101654, 0.4703215942335129, 0.5539687369918823, 0.5854696180915833, 0.5198580288219452, 0.5484057223129273, 0.6158625351619721, 0.5792768238639832, 0.5337661961746216, 0.5886052313423157, 0.54467154712677, 0.473427551279068, 0.4816957168865204, 0.5139731049251557, 0.550352405872345, 0.4698507158279419, 0.5226176941871643, 0.48719725017547605, 0.48560441466331483, 0.4822244606876373, 0.5956338468170166, 0.5404972162437439, 0.46963005029678345, 0.5527439107131958, 0.513271921749115, 0.4574300882911682, 0.4538339582061768, 0.47028755069732664, 0.42532012575149536, 0.4283977487182617, 0.4530713115692139, 0.4484299855709076, 0.46192191455841064, 0.4623718216705322, 0.47747744750022886, 0.4378578797149658, 0.44730199337005616, 0.4541953098487854, 0.4522034854507446, 0.4571830992126465, 0.4335714791870117, 0.427637976436615, 0.4394387770175934, 0.4353127297210693, 0.4287326905155182, 0.4174875314235687, 0.4283384783363342, 0.4249683612442017, 0.41909786179065706, 0.42239480409622193, 0.4236892115879059, 0.42108781467437745, 0.4160034220790863, 0.41670239393234254, 0.41843662134170534, 0.417412882938385, 0.4146304425907135, 0.420097362241745, 0.4165667124938965, 0.4135247115325928, 0.41516472861289977, 0.4136860141944885, 0.41217745824813845, 0.4159237167835236, 0.4151900646591187, 0.40994485832214356, 0.41669623952865603, 0.41205567766189577, 0.412605232963562, 0.41245238943099977, 0.41567705671310423, 0.41590032676696775, 0.41446701001167296, 0.4119553282356262, 0.412061138677597, 0.41358045135498045, 0.4162572134208679, 0.41225020356655123, 0.4152240486907959, 0.4137391041278839, 0.4160864718437195], 'train_acc1es': [32.86799998901367, 53.528000007324216, 62.39199997558594, 67.29599998291016, 70.92000001464844, 74.30399999511718, 76.35999998046876, 77.848, 79.05599998046875, 80.1439999975586, 81.26000001464844, 82.17599997314453, 82.22800001953125, 83.21200000488281, 83.74799997314453, 84.37599997314453, 84.82800001464844, 84.876, 85.21999999755859, 85.7, 85.77600000976562, 85.996, 86.67199998779297, 86.66000000244141, 86.87199999023437, 86.84000000488281, 87.42, 87.04400001708984, 87.7240000024414, 87.8480000048828, 87.74400000244141, 87.99199999511718, 88.35999999023437, 88.55199999511719, 88.55599998291015, 88.90799999755859, 88.88800000976562, 89.06000000488281, 89.30399998535157, 89.13599999511719, 89.29199999267578, 89.60800000488281, 89.7960000024414, 89.87599999511718, 89.94799998535156, 89.63200000976562, 90.468, 90.25999999511718, 90.17599998535157, 90.48399998046875, 90.256, 90.44799997070312, 90.67999998535156, 90.6239999975586, 90.95999999511719, 90.47199998779297, 91.05199999023438, 91.09199997802735, 91.31599998291016, 90.99999998535156, 91.65999999511719, 91.24399998046874, 91.51199998535157, 91.91599999267578, 92.03999997558594, 91.8519999975586, 92.0359999951172, 92.12000000488281, 92.23599998291016, 92.03199997558593, 92.15999997070313, 92.61599998779297, 92.39999999267579, 92.65599999023438, 92.66399997802735, 92.43599998779297, 92.73199998291015, 92.84399999267578, 92.98399998046875, 93.02399998291015, 93.29599997558594, 93.41199997802734, 93.61199997314453, 93.29599999023438, 93.79199998779296, 93.40799998046874, 94.04799997070313, 93.87199997314453, 93.85599998046875, 94.07199997314453, 94.29999997802734, 94.05999997558594, 94.37600001708985, 94.50799997314454, 94.46400000976563, 94.61200001464844, 94.27999997558594, 94.87199997070313, 95.18399997314454, 94.59999999023438, 95.19599997070313, 95.13599997314454, 95.35199997314453, 95.17599997558594, 95.31999997070312, 95.41199998291016, 95.57199998046875, 95.63600001464843, 95.59200001220704, 95.77999998046874, 95.89200001953125, 95.85599997314453, 95.98400001953125, 96.16800001464844, 96.29600001953125, 96.37200001220702, 96.30799997314453, 96.54400001464843, 96.46000001220703, 96.76799997070313, 97.09200001464843, 97.06399997558594, 97.15600001464844, 97.43200001464844, 97.29599997070312, 97.24400001464844, 97.59199997314452, 97.59600000732422, 97.64000000976563, 97.7280000024414, 97.64400001464844, 97.79200000732422, 97.98000001220703, 98.0720000024414, 98.28400001708984, 98.12800000244141, 98.21600000976562, 98.49200000976562, 98.50400001220703, 98.41600000976563, 98.56800000488282, 98.8600000048828, 98.85200000488281, 98.98800000244141, 99.08400000488281, 99.05600000244141, 99.12800000244141, 99.29600000488281, 99.18800000488281, 99.38800000732422, 99.39200000488282, 99.42000000488281, 99.60800000732422, 99.5080000024414, 99.536, 99.65200000488281, 99.592, 99.6560000024414, 99.732, 99.796, 99.7760000024414, 99.82, 99.816, 99.884, 99.9120000024414, 99.90800000244141, 99.892, 99.9000000024414, 99.928, 99.916, 99.948, 99.924, 99.972, 99.968, 99.964, 99.948, 99.964, 99.968, 99.952, 99.948, 99.96, 99.96, 99.984, 99.948, 99.984, 99.992, 99.964, 99.992, 99.964, 99.972, 99.976, 99.98, 99.984, 100.0, 99.988, 99.984, 99.984, 99.976, 99.976, 99.968], 'eval_acc1es': [41.03999999633789, 45.6880000012207, 57.227999998779296, 63.77199999023438, 62.38399999267578, 69.82399997558593, 70.60799998779297, 74.79199998046874, 72.80800001708984, 71.01599999267579, 75.71999997314452, 75.15999998046875, 66.86, 73.90399999511719, 78.76, 75.73600001708985, 75.01999998779297, 75.94400000976563, 77.948, 77.128, 78.49600001708984, 72.59999999023438, 80.14000001953124, 73.75600000976563, 81.30800001464844, 79.14799998779297, 73.75999998779297, 79.24399997802735, 74.55199998291016, 77.89599998779296, 78.79199999267578, 76.60399999023437, 80.02399997802735, 77.98399999511719, 80.54399998046875, 79.12399997070312, 80.11199997314453, 80.28399998779297, 77.16000000976562, 80.31199998535156, 79.8320000024414, 77.94800001464844, 80.63600001953125, 81.14799997802734, 80.65599997558594, 78.70799999511719, 78.72399998046875, 76.936, 81.69599998291015, 77.57199997558594, 81.30400001708985, 79.71199998779296, 83.00000000976563, 77.47599997070313, 82.50400001708984, 81.89600000732422, 79.752, 79.09599998046875, 81.71599998046875, 83.59599997070312, 83.55599997558593, 85.29599999267577, 82.30799997070312, 80.79200001953124, 81.04400001708984, 76.46000000976562, 82.97600000732422, 83.59599997802735, 78.57199999511718, 84.72800001220703, 80.96799999267579, 83.05199997070312, 80.77600001708984, 76.216, 82.832, 80.96800001953125, 80.05599998779297, 83.77200001220703, 81.68400001953125, 84.48800001953126, 77.98399998291016, 83.22799998535156, 80.58000000732422, 81.33599997802735, 83.29599999267577, 81.66000000244141, 83.95999998046875, 80.91999997070313, 82.90400001220704, 82.87600001220703, 83.94000000488282, 76.33599997802735, 85.28800000976563, 82.06400001708984, 84.24800001708985, 85.172, 84.23200000732422, 82.84000001708985, 82.76000001464844, 81.17599998535157, 84.16000000976562, 86.372, 84.9600000024414, 84.12800000732422, 85.32800001220703, 84.59600001708985, 84.35600000488282, 83.8560000024414, 85.38000000488282, 82.80400000732422, 85.43200001953124, 83.46800001708985, 82.72400001708985, 86.5519999975586, 85.53200000976562, 84.38800000976562, 86.41200000732422, 87.2719999975586, 86.27199997070312, 84.31200000488282, 87.14799998535156, 87.47599998779297, 86.25200000488282, 85.28400001220703, 86.26399999511719, 85.78000000976563, 84.824, 85.95600001220703, 86.45600000976563, 85.24800000732422, 86.38800000488281, 87.76399998046875, 87.66399999267578, 87.2159999975586, 86.19599999755859, 88.30799998291016, 87.296, 87.94799997802734, 88.15599998779297, 88.10799999267579, 86.59999999023438, 87.43599999023438, 88.64800001953125, 87.35200001464844, 87.81599999267578, 89.0279999975586, 89.04400000244141, 88.91199998291016, 89.42000000244141, 89.656, 89.24799999511718, 89.55599997802734, 89.372, 89.47599999511719, 89.17199998291015, 89.66400001220703, 89.89199999511719, 89.48799999267578, 89.65199999755859, 89.83199998779297, 90.0359999951172, 90.30799998779297, 89.94800000732423, 90.18799999755859, 90.49599998535156, 90.62399997070312, 90.38799998779297, 90.52000001220703, 90.73199998046876, 90.52799997558594, 90.53999998779297, 90.57599998779297, 90.80000001953125, 90.69999998291016, 90.7639999975586, 90.70399998046875, 90.82399999267578, 90.71599997070312, 90.76800000244141, 90.77599998291015, 90.85599998291016, 90.77599998291015, 90.75999998291016, 90.77999998535157, 90.83199998535156, 90.79599998046875, 90.80399999267578, 90.77599999023437, 90.73599999511718, 90.79199999023437, 90.78400001220703, 90.8, 90.76799998291015, 90.80399999023437, 90.81999998535156, 90.81999998535156, 90.78799999267578, 90.84400001708984, 90.78799999511719, 90.79199997070313, 90.8], 'cost_info': {'flops': 86.42625, 'params': 0.615386, 'latency': 0.019886672496795654, 'train_time': 11.101390759150187}}}\n",
            "\u001b[32m[05/15 12:54:24 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:24 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:24 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:25 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:25 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:25 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:25 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:25 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:25 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:26 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.80000, 0.00000\n",
            "\u001b[32m[05/15 12:54:26 nl.defaults.trainer]: \u001b[0mTraining finished\n"
          ]
        }
      ],
      "source": [
        "# call only a method to run the search for the number of iterations specified in the yaml configuration file.\n",
        "trainer.search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qnkRfsn9prT3",
        "outputId": "b347bb2a-4490-44c1-80ec-59d59c176699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[05/15 12:54:30 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[05/15 12:54:30 nl.defaults.trainer]: \u001b[0mloading model from file run/cifar10/nas_predictors/nasbench201/var_sparse_gp/0/search/model_final.pth\n",
            "\u001b[32m[05/15 12:54:30 nl.defaults.trainer]: \u001b[0mFinal architecture:\n",
            "Graph makrograph-0.7165554, scope None, 20 nodes\n",
            "\u001b[32m[05/15 12:54:30 nl.defaults.trainer]: \u001b[0mQueried results (Metric.TEST_ACCURACY): 90.8\n"
          ]
        }
      ],
      "source": [
        "# After the search is done, we want to evaluate the test performance of\n",
        "# the best architecture found using the validation set.\n",
        "trainer.evaluate(dataset_api=dataset_api)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS6hJNSoprT3"
      },
      "source": [
        "## NAS predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpXwUoSWprT3"
      },
      "source": [
        "The performance predictors in NASLib are categorized in 4 classes:\n",
        "- *Model-based predictors*\n",
        "    - These are usually regression models (e.g. Gaussian Processes or XGBoost) that are trained with a data (x, y), where x is the architecture encoding and y is the validation performance of the trained architectures. At test time, they predict the performance of new architectures from the space.\n",
        "- *Learning curve predictors*\n",
        "    - These predictors estimate the architecture ranking (e.g. at epoch 100) by using the performance at a lower epoch or some other statistics. An example is early stopping. By early stopping the training at epoch 30, we hope that the ranking of the architectures will be the same as the ranking at epoch 100.\n",
        "- *Zero-cost predictors*\n",
        "    - These predictors usually run only a single mini-batch iteration through a sampled architecture and use some statistics (e.g. norm of gradients) in order to determine how good that architecture is.\n",
        "- *One-shot predictors*\n",
        "    - This class of predictors utilizes the shared weights of the one-shot model in order to rank the architectures based on the validation performance using these shared weights.\n",
        "    \n",
        "In NASLib we have implemented **31 predictors** that can be easily imported and evaluated on the tabular benchmarks. Check also this file for some more details: https://github.com/automl/NASLib/blob/master/docs/predictors.md and our paper: https://arxiv.org/abs/2104.01177"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "5iXcThZDyaZu",
        "outputId": "6f77d95b-847a-4b0b-962b-ffbfe14ed200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NASLib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MOQKxzD2prT4",
        "outputId": "012aecff-9f7c-4c94-8f6e-cdc3b0efc087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['--config-file', '/content/NASLib/naslib/benchmarks/predictors/predictor_config.yaml']\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mCommand line args: ['--config-file', '/content/NASLib/naslib/benchmarks/predictors/predictor_config.yaml']\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mExperiment dir : run/cifar10/predictors/xgb/1000\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mExperiment dir : run/cifar10/predictors/xgb/1000/search\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mExperiment dir : run/cifar10/predictors/xgb/1000/eval\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mexperiment_type.............................single\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0msearch_space...........................nasbench201\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mdataset....................................cifar10\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mpredictor......................................xgb\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0muniform_random...................................1\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mtest_size......................................100\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mtrain_size_single...............................50\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mtrain_size_list...[5, 8, 14, 24, 42, 71, 121, 205]\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mfidelity_single..................................5\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mfidelity_list[1, 2, 3, 5, 7, 9, 13, 19, 26, 37, 52, 73]\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mout_dir........................................run\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mmax_hpo_time.....................................0\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mseed..........................................1000\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0msearchbatch_size: 256\n",
            "cutout: False\n",
            "cutout_length: 16\n",
            "cutout_prob: 1.0\n",
            "data_size: 25000\n",
            "seed: 1000\n",
            "train_portion: 0.7\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0msave...............run/cifar10/predictors/xgb/1000\n",
            "\u001b[32m[05/15 12:56:06 nl.utils.utils]: \u001b[0mdata.................../content/NASLib/naslib/data\n",
            "\u001b[32m[05/15 12:56:06 nl.defaults.predictor_evaluator]: \u001b[0mLoad the test set\n",
            "\u001b[32m[05/15 12:56:27 nl.defaults.predictor_evaluator]: \u001b[0mLoad the training set\n",
            "\u001b[32m[05/15 12:56:39 nl.defaults.predictor_evaluator]: \u001b[0mFit the predictor\n",
            "\u001b[32m[05/15 12:56:39 nl.defaults.predictor_evaluator]: \u001b[0mCompute evaluation metrics\n",
            "\u001b[32m[05/15 12:56:39 nl.defaults.predictor_evaluator]: \u001b[0mtrain_size: 50, fidelity: 5, kendall tau 0.3558\n",
            "\u001b[32m[05/15 12:56:39 nl.defaults.predictor_evaluator]: \u001b[0mmae: 4.3203, rmse: 8.3852, pearson: 0.3591, spearman: 0.4886, kendalltau: 0.3558, kt_2dec: 0.3557, kt_1dec: 0.3563, precision_10: 0.5, precision_20: 0.45, train_size: 50, fidelity: 5, train_time: 462.4131, fit_time: 0.1619, query_time: 0.0, hp_max_depth: 6, hp_min_child_weight: 1, hp_colsample_bytree: 1, hp_learning_rate: 0.3, hp_colsample_bylevel: 1, cv_score: 0, \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CfgNode({'experiment_type': 'single', 'search_space': 'nasbench201', 'dataset': 'cifar10', 'predictor': 'xgb', 'uniform_random': 1, 'test_size': 100, 'train_size_single': 50, 'train_size_list': [5, 8, 14, 24, 42, 71, 121, 205], 'fidelity_single': 5, 'fidelity_list': [1, 2, 3, 5, 7, 9, 13, 19, 26, 37, 52, 73], 'out_dir': 'run', 'max_hpo_time': 0, 'seed': 1000, 'search': CfgNode({'seed': 1000, 'batch_size': 256, 'data_size': 25000, 'cutout': False, 'cutout_length': 16, 'cutout_prob': 1.0, 'train_portion': 0.7}), 'save': 'run/cifar10/predictors/xgb/1000', 'data': '/content/NASLib/naslib/data'}),\n",
              " {'cv_score': 0,\n",
              "  'fidelity': 5,\n",
              "  'fit_time': 0.16188526153564453,\n",
              "  'hp_booster': 'gbtree',\n",
              "  'hp_colsample_bylevel': 1,\n",
              "  'hp_colsample_bytree': 1,\n",
              "  'hp_eval_metric': 'rmse',\n",
              "  'hp_learning_rate': 0.3,\n",
              "  'hp_max_depth': 6,\n",
              "  'hp_min_child_weight': 1,\n",
              "  'hp_objective': 'reg:squarederror',\n",
              "  'kendalltau': 0.35577120352397784,\n",
              "  'kt_1dec': 0.3563206034824803,\n",
              "  'kt_2dec': 0.35567687978766804,\n",
              "  'mae': 4.320324826049804,\n",
              "  'pearson': 0.35911963551925935,\n",
              "  'precision_10': 0.5,\n",
              "  'precision_20': 0.45,\n",
              "  'query_time': 1.5289783477783202e-05,\n",
              "  'rmse': 8.385226457743993,\n",
              "  'spearman': 0.48864365998706494,\n",
              "  'train_size': 50,\n",
              "  'train_time': 462.41313511133194}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Load the predictor evaluator and the predictor (XGBoost in this case)\n",
        "from naslib.defaults.predictor_evaluator import PredictorEvaluator\n",
        "from naslib.predictors import XGBoost\n",
        "import os\n",
        "\n",
        "# read the new configuration file that has the parameters of the predictor model\n",
        "# NOTE: it is important to set config_type=\"predictor\" here\n",
        "config = utils.get_config_from_args(args=[\"--config-file\", os.getcwd()+\"/naslib/benchmarks/predictors/predictor_config.yaml\"], \n",
        "                                    config_type=\"predictor\")\n",
        "utils.set_seed(config.seed)\n",
        "utils.log_args(config)\n",
        "\n",
        "logger = setup_logger(config.save + \"/log.log\")\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Now instantiate the predictor (every predictor works with certain encoding types for the architecture)\n",
        "predictor = XGBoost(encoding_type='adjacency_one_hot', hpo_wrapper=False)\n",
        "# Instantiate the evaluator\n",
        "predictor_evaluator = PredictorEvaluator(predictor, config=config)\n",
        "# similarly to the conventional NAS search that we saw before, the predictor evaluator also adapts to \n",
        "# the search space at hand\n",
        "predictor_evaluator.adapt_search_space(search_space, load_labeled=False, \n",
        "                                       dataset_api=dataset_api)\n",
        "# No search in this case. We only train the predictor on the training data and evaluate it on the test data.\n",
        "# Note that the training data here is the pair (arch, performance) and not (image, label).\n",
        "predictor_evaluator.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nou1eDUxprT4"
      },
      "source": [
        "The stdout shows some metrics such as the rank correlation (Spearman or Kendall Tau) of 100 sampled architectures from the test set. This shows how good the ranking of the architectures based on the performance predictor is compared to the true ranking from NAS-Bench-201."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksrXqJoprT4"
      },
      "source": [
        "## Using the predictors as surrogate models in Bayesian Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvLIQmuoprT4"
      },
      "source": [
        "In order to use the aforementioned predictors as surrogate models inside Bayesian Optimization, we need to have a mean prediction and uncertainty estimates for every architecture. Some of the models (e.g. GPs) already provide this, but for some others such as MLPs we construct an ensemble of MLPs in order to obtain the uncertainty estimates.\n",
        "\n",
        "This code snippet shows how to run [BANANAS](https://arxiv.org/abs/1910.11858) with some of the performance predictors as surrogate models. We will do 3 trials with 3 different seeds of BANANAS for 300 iterations. For this we need to firstly generate the configuration files. The bash commands below do this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8S7xeeIeprT4",
        "outputId": "4f9352fc-8b24-434d-c970-62a39ec1612c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "python3: can't open file '../naslib/benchmarks/create_configs.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '../naslib/benchmarks/create_configs.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '../naslib/benchmarks/create_configs.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '../naslib/benchmarks/create_configs.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '../naslib/benchmarks/create_configs.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '../naslib/benchmarks/create_configs.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "optimizer=bananas\n",
        "predictors=(mlp lgb xgb rf bayes_lin_reg gp)\n",
        "\n",
        "start_seed=0\n",
        "\n",
        "# folders:\n",
        "# this supposes your location is at NASLib/docs. Change the base_file location based on where you\n",
        "# opened the notebook\n",
        "base_file=../naslib\n",
        "save_dir=bananas_run\n",
        "out_dir=$save_dir\\_$start_seed\n",
        "\n",
        "# search space / data:\n",
        "search_space=nasbench201\n",
        "dataset=cifar10\n",
        "search_epochs=300\n",
        "\n",
        "# trials / seeds:\n",
        "trials=3\n",
        "end_seed=$(($start_seed + $trials - 1))\n",
        "\n",
        "# create config files\n",
        "for i in $(seq 0 $((${#predictors[@]}-1)) )\n",
        "do\n",
        "    predictor=${predictors[$i]}\n",
        "    python $base_file/benchmarks/create_configs.py --predictor $predictor \\\n",
        "    --epochs $search_epochs --start_seed $start_seed --trials $trials \\\n",
        "    --out_dir $out_dir --dataset=$dataset --config_type nas_predictor \\\n",
        "    --search_space $search_space --optimizer $optimizer\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhF01clEprT5"
      },
      "source": [
        "Similarly to how we ran RE before, write a function that gets a configuration file and optimizer as input and runs them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0K4I2ivprT5"
      },
      "outputs": [],
      "source": [
        "from naslib.optimizers import Bananas\n",
        "\n",
        "def run_optimizer(config_file=\"../docs/bananas_run_0/cifar10/configs/nas_predictors/config_bananas_gp_0.yaml\",\n",
        "                  nas_optimizer=Bananas) -> None:\n",
        "    # TODO: add all the utilities, such as config file reading, logging as before.\n",
        "    # afterwards instantiate the search space, optimizer, trainer and run the search + evaluation\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h00K6A7NprT5"
      },
      "outputs": [],
      "source": [
        "run_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEkCWKy2prT5"
      },
      "source": [
        "# TASK: Implement a Regularized Evolution version that uses the performance predictors as surrogate models during the search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2TCQHlprT5"
      },
      "source": [
        "The Regularized Evolution (RE) code that we ran above uses the true validation performance as queried from NAS-Bench-201. Using the tabular benchmark this is certainly cheap since we are only simulating the true run. However, in real world scenarios these tabular entries are not available so we have to train every sampled/mutated architecture from scratch. This is the most expensive step in black-box optimization for NAS.\n",
        "\n",
        "The NAS performance predictors can drastically accelerate the search by utilizing the performance as predicted by the surrogate model (performance predictor). In this exercise you will have to implement a RE version that utilizes the performance predictors to estimate the performance of the sampled architectures instead of querying that from the tabular benchmark. Sample the very first architectures in the population and query those from NB201. Then fit the performance predictor. Afterwards, every 10 iterations query again the performance from NB201 and refit the predictor. In between, however, use the performance returned by the predictor inside RE.\n",
        "\n",
        "HINT: Check how the performance predictors are utilized inside the BANANAS implementation in NASLib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR1Nxb44prT5"
      },
      "outputs": [],
      "source": [
        "class RE_predictor(RE):\n",
        "    def __init__(self, config):\n",
        "        # you probably would need to add some more attributes to the __init__ method\n",
        "        return NotImplementedError\n",
        "    \n",
        "    def new_epoch(self, epoch):\n",
        "        # This is the main method that you have to override in order to add the performance predictors\n",
        "        return NotImplementedError\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylZKDbrUprT6"
      },
      "source": [
        "Now generate the new yaml configuration files using the bash commands as shown above. Just change \"bananas\" to \"re\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dprjllKZprT6"
      },
      "outputs": [],
      "source": [
        "#TODO: Run the RE_predictor optimizer\n",
        "\n",
        "# for config_file in list_of_config_files:\n",
        "#    run_optimizer(config_file=config_file, nas_optimizer=RE_predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMFVmH7HprT6"
      },
      "source": [
        "### Plotting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XKFU4CcprT6"
      },
      "source": [
        "The results should have been written to `re_run_0/cifar10/nas_predictors/nasbench201`. Use the other jupyter notebook located in `NASLib/naslib/docs/plot.ipynb` to generate the plots based on these results."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "naslib_tutorial.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}